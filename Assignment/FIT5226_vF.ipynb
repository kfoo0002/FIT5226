{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40d40f18-18f0-4900-8ec0-79f11ae5e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and Agent Definitions\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define agent class to hold state\n",
    "class QTableAgent:\n",
    "    def __init__(self, agent_id):\n",
    "        self.agent_id = agent_id\n",
    "        self.has_item = 0        # 1 if carrying an item, else 0\n",
    "        self.direction = None    # True if heading A->B, False if heading B->A\n",
    "        self.position = None\n",
    "        self.previous_position = None\n",
    "        self.local_mask = 0      # 8-bit mask for opposite-direction neighbors\n",
    "        self.update_order = None # order index for central clock updates\n",
    "        self.collision_penalty = 0\n",
    "        self.completed_round_trip = False\n",
    "\n",
    "# Grid world environment for multi-agent shuttle\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, n=5, m=5, num_agents=4, food_source_location=None, nest_location=None):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.num_agents = num_agents\n",
    "        # Set A (food source) and B (nest) locations\n",
    "        self.food_source_location = food_source_location if food_source_location is not None else (0, 0)\n",
    "        self.nest_location = nest_location if nest_location is not None else (n-1, m-1)\n",
    "        # Initialize agents list\n",
    "        self.agents = [QTableAgent(i) for i in range(num_agents)]\n",
    "        # Define relative positions for the 8 neighboring directions (N, NE, E, SE, S, SW, W, NW)\n",
    "        self.directions = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                            (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        # Central clock for coordinated updates\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(num_agents))  # default round-robin order\n",
    "        # Collision and round-trip tracking\n",
    "        self.collision_count = 0\n",
    "        self.collision_penalty_value = -50  # penalty applied to each agent in a collision\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * num_agents       # progress state (0->3) for each agent's round trip\n",
    "        self.agent_round_trips = [False] * num_agents  # whether each agent completed a round trip\n",
    "        # Place agents at start positions\n",
    "        self._reset()  # randomize initial agent placements at A or B\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset agents to random start at A or B, and reset environment counters.\"\"\"\n",
    "        # Randomize each agent's starting position: 50% at A (with item), 50% at B (empty)\n",
    "        for agent in self.agents:\n",
    "            if random.random() < 0.5:\n",
    "                agent.position = self.food_source_location\n",
    "                agent.has_item = 1\n",
    "                agent.direction = True   # carrying item, so heading toward B\n",
    "            else:\n",
    "                agent.position = self.nest_location\n",
    "                agent.has_item = 0\n",
    "                agent.direction = False  # not carrying, so heading toward A\n",
    "            agent.previous_position = None\n",
    "            agent.local_mask = 0\n",
    "            agent.collision_penalty = 0\n",
    "            agent.completed_round_trip = False\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(self.num_agents))\n",
    "\n",
    "    \n",
    "    def reset_environment_counters(self):\n",
    "        \"\"\"Reset only environment counters, not agent positions\"\"\"\n",
    "        self.collision_count = 0\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * self.num_agents\n",
    "        self.agent_round_trips = [False] * self.num_agents\n",
    "\n",
    "    def get_next_agent(self):\n",
    "        \"\"\"Return the next agent ID to act, based on the central clock (round-robin).\"\"\"\n",
    "        agent_id = self.update_sequence[self.clock % self.num_agents]\n",
    "        self.clock += 1\n",
    "        return agent_id\n",
    "\n",
    "    def get_local_mask(self, agent_id):\n",
    "        \"\"\"\n",
    "        Compute an 8-bit mask for agent agent_id indicating which of the 8 neighboring cells\n",
    "        contain an agent moving in the opposite direction (potential head-on collision threat).\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        x, y = agent.position\n",
    "        mask = 0\n",
    "        # Check all neighbors\n",
    "        for i, (dx, dy) in enumerate(self.directions):\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.n and 0 <= ny < self.m:\n",
    "                for other in self.agents:\n",
    "                    if other.agent_id != agent_id:\n",
    "                        if other.position == (nx, ny) and other.direction != agent.direction:\n",
    "                            # Neighbor cell occupied by an opposite-direction agent\n",
    "                            mask |= (1 << i)\n",
    "        return mask\n",
    "\n",
    "    def get_state(self, agent_id):\n",
    "        \"\"\"Return the 15-dimensional state vector for the given agent.\"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Update the agent's local mask (for visualization/debugging if needed)\n",
    "        agent.local_mask = self.get_local_mask(agent_id)\n",
    "        # Construct state: position (x,y), A coords, B coords, carry flag, and 8 neighbor flags\n",
    "        x, y = agent.position\n",
    "        state = [\n",
    "            x, y,\n",
    "            self.food_source_location[0], self.food_source_location[1],\n",
    "            self.nest_location[0], self.nest_location[1],\n",
    "            agent.has_item\n",
    "        ]\n",
    "        # Append 8 binary flags indicating opposite-direction agents around\n",
    "        for i in range(8):\n",
    "            flag = (agent.local_mask >> i) & 1\n",
    "            state.append(flag)\n",
    "        return state\n",
    "\n",
    "    def check_collisions(self):\n",
    "        \"\"\"\n",
    "        Detect head-on collisions: if any non-endpoint cell is occupied by at least one agent with an item \n",
    "        and at least one without an item (i.e., agents heading in opposite directions). \n",
    "        Apply collision penalty to those agents.\n",
    "        \"\"\"\n",
    "        positions = {}\n",
    "        # Group agents by their current position\n",
    "        for agent in self.agents:\n",
    "            positions.setdefault(agent.position, []).append(agent)\n",
    "        # Check each position that has more than one agent\n",
    "        for pos, agents in positions.items():\n",
    "            if len(agents) < 2:\n",
    "                continue\n",
    "            # Ignore collisions at A or B (agents clustering at endpoints is allowed)\n",
    "            if pos == self.food_source_location or pos == self.nest_location:\n",
    "                continue\n",
    "            # Check if there's at least one carrying and one not carrying agent\n",
    "            carrying = any(a.has_item for a in agents)\n",
    "            not_carrying = any(not a.has_item for a in agents)\n",
    "            if carrying and not_carrying:\n",
    "                # Head-on collision detected\n",
    "                self.collision_count += 1\n",
    "                for agent in agents:\n",
    "                    agent.collision_penalty += self.collision_penalty_value\n",
    "\n",
    "    def take_action(self, agent_id, action):\n",
    "        \"\"\"\n",
    "        Execute the given action for agent agent_id. \n",
    "        Moves the agent, applies pickup/dropoff, updates round-trip state, and returns the reward.\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Save previous position (for potential collision checks or info)\n",
    "        agent.previous_position = agent.position\n",
    "        x, y = agent.position\n",
    "        # Determine next position based on action\n",
    "        if action == 0:   # Up\n",
    "            nx, ny = max(x - 1, 0), y\n",
    "        elif action == 1: # Down\n",
    "            nx, ny = min(x + 1, self.n - 1), y\n",
    "        elif action == 2: # Left\n",
    "            nx, ny = x, max(y - 1, 0)\n",
    "        elif action == 3: # Right\n",
    "            nx, ny = x, min(y + 1, self.m - 1)\n",
    "        else:\n",
    "            nx, ny = x, y  # no-op for safety (should not happen)\n",
    "        next_pos = (nx, ny)\n",
    "        # Default step penalty\n",
    "        reward = -1\n",
    "        # Check for pickup or drop-off events\n",
    "        if next_pos == self.food_source_location and agent.has_item == 0:\n",
    "            # Arriving at A without an item -> pick up item\n",
    "            agent.has_item = 1\n",
    "            reward += 40   # net +39 for pickup (reward was -1, now -1+40)\n",
    "        elif next_pos == self.nest_location and agent.has_item == 1:\n",
    "            # Arriving at B with an item -> drop off\n",
    "            agent.has_item = 0\n",
    "            reward += 60   # net +59 for successful delivery\n",
    "        # Update agent's position\n",
    "        agent.position = next_pos\n",
    "        # Update agent's travel direction based on carry status\n",
    "        agent.direction = True if agent.has_item else False\n",
    "\n",
    "        # Update agent's round-trip progress state machine:\n",
    "        # States: 0 (not started or reset), 1 (halfway), 2 (reached opposite end), 3 (completed round trip).\n",
    "        # A -> B -> A trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 1  # Picked up at A, heading to B\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 2  # Dropped off at B (halfway done A->B->A)\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.food_source_location and agent.has_item == 0:\n",
    "            # Came back to A without item -> completed A->B->A round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        # B -> A -> B trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 1  # Started at B, heading to A\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 2  # Picked up at A, heading back to B\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            # Returned to B with item delivered -> completed B->A->B round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2df5fecf-4c58-4b08-abd1-a22444f17cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent Setup and Replay Buffer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Replay buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            # Remove oldest experience if at capacity\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states      = np.array([exp[0] for exp in batch], dtype=np.float32)\n",
    "        actions     = np.array([exp[1] for exp in batch], dtype=np.int64)\n",
    "        rewards     = np.array([exp[2] for exp in batch], dtype=np.float32)\n",
    "        next_states = np.array([exp[3] for exp in batch], dtype=np.float32)\n",
    "        dones       = np.array([exp[4] for exp in batch], dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize DQN policy network and target network\n",
    "state_dim = 15   # state features (from environment state vector)\n",
    "action_dim = 4   # four possible actions\n",
    "policy_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "target_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "# Copy weights from policy to target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# DQN hyperparameters\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.95             # discount factor\n",
    "batch_size = 64\n",
    "target_update_freq = 250 # how often to sync target net (in updates)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Epsilon-greedy exploration schedule\n",
    "class EpsilonScheduler:\n",
    "    def __init__(self, start=1.0, end=0.1, decay=0.995):\n",
    "        self.epsilon = start\n",
    "        self.min_epsilon = end\n",
    "        self.decay = decay\n",
    "    def get_epsilon(self):\n",
    "        # Return current epsilon and decay it for next call\n",
    "        eps = self.epsilon\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.decay)\n",
    "        return eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee3e26-6f8c-4251-8248-00458bdac9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Ep    1- 100 | Epsilon 0.61 | Reward   -52596 | Coll  326 | RT  100 | AvgLoss 243.7431 | Steps 5,496 | CumColl 326 | Time    7.2s\n",
      "Ep  101- 200 | Epsilon 0.37 | Reward   -27460 | Coll  208 | RT  100 | AvgLoss 304.8582 | Steps 9,416 | CumColl 534 | Time   12.3s\n",
      "Ep  201- 300 | Epsilon 0.22 | Reward    -9909 | Coll  130 | RT  100 | AvgLoss 313.4476 | Steps 12,715 | CumColl 664 | Time   16.7s\n",
      "Ep  301- 400 | Epsilon 0.14 | Reward    -4041 | Coll   97 | RT  100 | AvgLoss 315.2318 | Steps 15,596 | CumColl 761 | Time   20.5s\n",
      "Ep  401- 500 | Epsilon 0.08 | Reward      -56 | Coll   78 | RT   99 | AvgLoss 298.3016 | Steps 19,052 | CumColl 839 | Time   25.2s\n",
      "Ep  501- 600 | Epsilon 0.05 | Reward     2992 | Coll   72 | RT  100 | AvgLoss 293.3716 | Steps 21,740 | CumColl 911 | Time   28.8s\n",
      "Ep  601- 700 | Epsilon 0.05 | Reward    10496 | Coll   45 | RT  100 | AvgLoss 289.9249 | Steps 24,174 | CumColl 956 | Time   31.7s\n",
      "Ep  701- 800 | Epsilon 0.05 | Reward    12188 | Coll   31 | RT  100 | AvgLoss 275.4980 | Steps 26,146 | CumColl 987 | Time   33.9s\n",
      "Ep  801- 900 | Epsilon 0.05 | Reward     8352 | Coll   49 | RT  100 | AvgLoss 266.2720 | Steps 28,744 | CumColl 1,036 | Time   36.9s\n",
      "Ep  901-1000 | Epsilon 0.05 | Reward    12900 | Coll   38 | RT  100 | AvgLoss 262.2153 | Steps 31,554 | CumColl 1,074 | Time   39.9s\n",
      "Ep 1001-1100 | Epsilon 0.05 | Reward     7814 | Coll   67 | RT  100 | AvgLoss 249.5219 | Steps 34,010 | CumColl 1,141 | Time   42.7s\n",
      "Ep 1101-1200 | Epsilon 0.05 | Reward     7974 | Coll   68 | RT  100 | AvgLoss 248.3090 | Steps 36,486 | CumColl 1,209 | Time   45.5s\n",
      "Ep 1201-1300 | Epsilon 0.05 | Reward    10771 | Coll   49 | RT  100 | AvgLoss 243.9579 | Steps 39,075 | CumColl 1,258 | Time   48.3s\n",
      "Ep 1301-1400 | Epsilon 0.05 | Reward    14319 | Coll   36 | RT  100 | AvgLoss 241.8557 | Steps 41,346 | CumColl 1,294 | Time   50.8s\n",
      "Ep 1401-1500 | Epsilon 0.05 | Reward    12096 | Coll   48 | RT  100 | AvgLoss 235.4792 | Steps 43,550 | CumColl 1,342 | Time   53.4s\n",
      "Ep 1501-1600 | Epsilon 0.05 | Reward     8850 | Coll   54 | RT  100 | AvgLoss 234.8290 | Steps 45,610 | CumColl 1,396 | Time   55.6s\n",
      "Ep 1601-1700 | Epsilon 0.05 | Reward    12250 | Coll   60 | RT  100 | AvgLoss 235.0416 | Steps 47,850 | CumColl 1,456 | Time   58.1s\n",
      "Ep 1701-1800 | Epsilon 0.05 | Reward     7888 | Coll   64 | RT  100 | AvgLoss 238.6747 | Steps 50,172 | CumColl 1,520 | Time   60.7s\n",
      "Ep 1801-1900 | Epsilon 0.05 | Reward    13100 | Coll   35 | RT  100 | AvgLoss 233.0254 | Steps 52,582 | CumColl 1,555 | Time   63.4s\n",
      "Ep 1901-2000 | Epsilon 0.05 | Reward    10159 | Coll   59 | RT  100 | AvgLoss 226.1491 | Steps 54,863 | CumColl 1,614 | Time   65.9s\n",
      "Ep 2001-2100 | Epsilon 0.05 | Reward    15655 | Coll   27 | RT  100 | AvgLoss 219.4985 | Steps 57,178 | CumColl 1,641 | Time   68.7s\n",
      "Ep 2101-2200 | Epsilon 0.05 | Reward    13852 | Coll   36 | RT  100 | AvgLoss 211.3471 | Steps 59,556 | CumColl 1,677 | Time   71.4s\n",
      "Ep 2201-2300 | Epsilon 0.05 | Reward    15490 | Coll   48 | RT  100 | AvgLoss 198.6920 | Steps 62,156 | CumColl 1,725 | Time   74.5s\n",
      "Ep 2301-2400 | Epsilon 0.05 | Reward    15366 | Coll   29 | RT  100 | AvgLoss 195.5879 | Steps 64,490 | CumColl 1,754 | Time   77.3s\n",
      "Ep 2401-2500 | Epsilon 0.05 | Reward     8178 | Coll   72 | RT  100 | AvgLoss 195.6008 | Steps 66,792 | CumColl 1,826 | Time   80.0s\n",
      "Ep 2501-2600 | Epsilon 0.05 | Reward    14561 | Coll   46 | RT  100 | AvgLoss 191.2687 | Steps 69,071 | CumColl 1,872 | Time   82.8s\n",
      "Ep 2601-2700 | Epsilon 0.05 | Reward    12902 | Coll   40 | RT  100 | AvgLoss 186.4073 | Steps 71,669 | CumColl 1,912 | Time   86.0s\n",
      "Ep 2701-2800 | Epsilon 0.05 | Reward    15629 | Coll   33 | RT  100 | AvgLoss 182.3057 | Steps 74,050 | CumColl 1,945 | Time   88.7s\n",
      "Ep 2801-2900 | Epsilon 0.05 | Reward    18709 | Coll   22 | RT  100 | AvgLoss 176.3692 | Steps 76,421 | CumColl 1,967 | Time   91.3s\n",
      "Ep 2901-3000 | Epsilon 0.05 | Reward    13516 | Coll   37 | RT  100 | AvgLoss 178.7733 | Steps 78,615 | CumColl 2,004 | Time   93.7s\n",
      "Ep 3001-3100 | Epsilon 0.05 | Reward    12899 | Coll   43 | RT  100 | AvgLoss 185.7450 | Steps 80,856 | CumColl 2,047 | Time   96.3s\n",
      "Ep 3101-3200 | Epsilon 0.05 | Reward    19795 | Coll   17 | RT  100 | AvgLoss 172.4320 | Steps 83,121 | CumColl 2,064 | Time   98.8s\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Logging utility to record training metrics\n",
    "class MetricLogger:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.collisions = []\n",
    "        self.round_trips = []\n",
    "        self.lengths = []\n",
    "    def log_episode(self, total_reward, collisions, round_trips, length):\n",
    "        self.rewards.append(total_reward)\n",
    "        self.collisions.append(collisions)\n",
    "        self.round_trips.append(round_trips)\n",
    "        self.lengths.append(length)\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"mean_reward\": np.mean(self.rewards) if self.rewards else 0.0,\n",
    "            \"mean_collisions\": np.mean(self.collisions) if self.collisions else 0.0,\n",
    "            \"mean_round_trips\": np.mean(self.round_trips) if self.round_trips else 0.0,\n",
    "            \"mean_length\": np.mean(self.lengths) if self.lengths else 0.0\n",
    "        }\n",
    "\n",
    "# Training parameters and budgets\n",
    "num_episodes = 5000\n",
    "max_steps_per_episode = 120   # safety cap on steps per episode (will often terminate earlier)\n",
    "buffer_capacity = 50000\n",
    "step_budget = 1_500_000       # max training steps (across all agents)\n",
    "collision_budget = 4000       # max total collisions during training\n",
    "walltime_budget = 600         # max wall-clock time (seconds) for training\n",
    "\n",
    "# Initialize environment, replay buffer, epsilon scheduler, and metric logger\n",
    "env = GridWorldEnvironment()\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "eps_scheduler = EpsilonScheduler(start=1.0, end=0.05, decay=0.995)\n",
    "logger = MetricLogger()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "# Training loop\n",
    "import time\n",
    "start_time = time.time()\n",
    "total_steps = 0\n",
    "total_collisions = 0\n",
    "\n",
    "# ------------ 100â€‘episode window accumulators ------------\n",
    "window_reward_sum     = 0.0\n",
    "window_coll_sum       = 0\n",
    "window_roundtrip_sum  = 0\n",
    "window_loss_sum       = 0.0\n",
    "window_loss_count     = 0\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    while True:\n",
    "        a_row = np.random.randint(env.n)\n",
    "        a_col = np.random.randint(env.m)\n",
    "        b_row = np.random.randint(env.n)\n",
    "        b_col = np.random.randint(env.m)\n",
    "        if (a_row, a_col) != (b_row, b_col):\n",
    "            break\n",
    "    env.food_source_location = (a_row, a_col)\n",
    "    env.nest_location = (b_row, b_col)\n",
    "    env.reset_environment_counters()\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_collisions = 0\n",
    "    episode_round_trips = 0\n",
    "    eps = eps_scheduler.get_epsilon()\n",
    "    done_episode = False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        for _ in range(env.num_agents):\n",
    "            agent_id = env.get_next_agent()\n",
    "            state = env.get_state(agent_id)\n",
    "\n",
    "            if random.random() < eps:\n",
    "                action = random.randrange(action_dim)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = int(torch.argmax(q_values).item())\n",
    "\n",
    "            reward = env.take_action(agent_id, action)\n",
    "            env.check_collisions()\n",
    "\n",
    "            if env.collision_count > episode_collisions:\n",
    "                new_collisions = env.collision_count - episode_collisions\n",
    "                total_collisions += new_collisions\n",
    "                episode_collisions = env.collision_count\n",
    "                for ag in env.agents:\n",
    "                    if ag.collision_penalty != 0:\n",
    "                        if ag.agent_id != agent_id:\n",
    "                            reward += ag.collision_penalty\n",
    "                        episode_reward += ag.collision_penalty\n",
    "                        ag.collision_penalty = 0\n",
    "\n",
    "            next_state = env.get_state(agent_id)\n",
    "            done = False\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                state_batch = torch.tensor(states, dtype=torch.float32)\n",
    "                next_state_batch = torch.tensor(next_states, dtype=torch.float32)\n",
    "                action_batch = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "                reward_batch = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "                done_batch = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                q_current = policy_net(state_batch).gather(1, action_batch)\n",
    "                q_next = target_net(next_state_batch).max(dim=1, keepdim=True)[0]\n",
    "                q_target = reward_batch + gamma * q_next * (1 - done_batch)\n",
    "\n",
    "                loss = loss_fn(q_current, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track loss in 100-episode window\n",
    "                window_loss_sum += loss.item()\n",
    "                window_loss_count += 1\n",
    "\n",
    "            total_steps += 1\n",
    "            if env.round_trip_count > 0:\n",
    "                done_episode = True\n",
    "            if total_steps >= step_budget or total_collisions >= collision_budget or (time.time() - start_time) >= walltime_budget:\n",
    "                done_episode = True\n",
    "            if done_episode:\n",
    "                break\n",
    "\n",
    "        if total_steps % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done_episode or step == max_steps_per_episode - 1:\n",
    "            episode_round_trips = env.round_trip_count\n",
    "            break\n",
    "\n",
    "    logger.log_episode(episode_reward, episode_collisions, episode_round_trips, step+1)\n",
    "\n",
    "    # --- Accumulate episode stats for current 100-episode window ---\n",
    "    window_reward_sum    += episode_reward\n",
    "    window_coll_sum      += episode_collisions\n",
    "    window_roundtrip_sum += episode_round_trips\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    # --- Print block every 100 episodes ---\n",
    "    if episode % 100 == 0:\n",
    "        avg_loss = (window_loss_sum / window_loss_count) if window_loss_count else float('nan')\n",
    "        print(f\"Ep {episode-99:4d}-{episode:4d} | \"\n",
    "              f\"Epsilon {eps:.2f} | \"\n",
    "              f\"Reward {window_reward_sum:8.0f} | \"\n",
    "              f\"Coll {window_coll_sum:4d} | \"\n",
    "              f\"RT {window_roundtrip_sum:4d} | \"\n",
    "              f\"AvgLoss {avg_loss:7.4f} | \"\n",
    "              f\"Steps {total_steps:,} | CumColl {total_collisions:,} | \"\n",
    "              f\"Time {time.time()-start_time:6.1f}s\") \n",
    "        # Reset accumulators\n",
    "        window_reward_sum     = 0.0 #\n",
    "        window_coll_sum       = 0\n",
    "        window_roundtrip_sum  = 0\n",
    "        window_loss_sum       = 0.0\n",
    "        window_loss_count     = 0\n",
    "\n",
    "    # Check stop conditions\n",
    "    if total_steps >= step_budget:\n",
    "        print(\"Stopping training early due to step budget limit reached.\")\n",
    "        break\n",
    "    elif total_collisions >= collision_budget:\n",
    "        print (\"Stopping training early due to collision budget limit reached.\")\n",
    "        break\n",
    "    elif (time.time() - start_time) >= walltime_budget:\n",
    "        print (\"Stopping training early due to time budget limit reached.\")\n",
    "        break\n",
    "\n",
    "# Save trained model\n",
    "torch.save(policy_net.state_dict(), \"trained_model.pth\")\n",
    "print(\"Training completed.\")\n",
    "stats = logger.get_stats()\n",
    "print(f\"Average Reward: {stats['mean_reward']:.2f}, \"\n",
    "      f\"Average Collisions: {stats['mean_collisions']:.2f}, \"\n",
    "      f\"Average RoundTrips: {stats['mean_round_trips']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15c5b962-72a4-458c-8bee-b383e0278f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 79.08%\n",
      "Success rate per distribution: {'1,3': 467, '2,2': 468, '3,1': 467, '4,0': 496}\n",
      "Failures breakdown: {'collisions': 79, 'timeout': 423, 'incomplete': 0}\n",
      "Average steps for successful episodes: 19.76\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of Trained Policy\n",
    "\n",
    "# Load the trained model for evaluation\n",
    "trained_model = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "trained_model.load_state_dict(torch.load(\"trained_model.pth\", map_location=\"cpu\"))\n",
    "trained_model.eval()\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"Evaluate the trained model on all possible A-B configurations and specified agent distributions.\"\"\"\n",
    "    GRID_SIZE = 5\n",
    "    distributions = [(1,3), (2,2), (3,1), (4,0)]  # (agents at B, agents at A)\n",
    "    success_count = 0\n",
    "    total_tests = 0\n",
    "    stats = {\n",
    "        'total_tests': 0,\n",
    "        'successful_tests': 0,\n",
    "        'failures': {'collisions': 0, 'timeout': 0, 'incomplete': 0},\n",
    "        'distribution_success': {'1,3': 0, '2,2': 0, '3,1': 0, '4,0': 0},\n",
    "        'avg_steps_successful': 0\n",
    "    }\n",
    "    # Helper: choose greedy action using the trained model\n",
    "    def greedy_action(state):\n",
    "        state_t = torch.tensor(state, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            q_vals = model(state_t)\n",
    "        return int(torch.argmax(q_vals).item())\n",
    "\n",
    "    # Iterate over all possible A, B placements\n",
    "    for ax in range(GRID_SIZE):\n",
    "        for ay in range(GRID_SIZE):\n",
    "            for bx in range(GRID_SIZE):\n",
    "                for by in range(GRID_SIZE):\n",
    "                    if (ax, ay) == (bx, by):\n",
    "                        continue  # skip invalid configs where A == B\n",
    "                    for (b_agents, a_agents) in distributions:\n",
    "                        total_tests += 1\n",
    "                        stats['total_tests'] = total_tests\n",
    "                        # Initialize env with given A and B\n",
    "                        test_env = GridWorldEnvironment(n=GRID_SIZE, m=GRID_SIZE, num_agents=4,\n",
    "                                                        food_source_location=(ax, ay),\n",
    "                                                        nest_location=(bx, by))\n",
    "                        # Manually set agents at B or A according to the distribution\n",
    "                        # First agent (index 0) at B, since we test that agent's ability to do B->A->B\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            if i < b_agents:\n",
    "                                # place this agent at B (nest), without item\n",
    "                                test_env.agents[i].position = test_env.nest_location\n",
    "                                test_env.agents[i].has_item = 0\n",
    "                                test_env.agents[i].direction = False\n",
    "                            else:\n",
    "                                # place this agent at A (food source), with item\n",
    "                                test_env.agents[i].position = test_env.food_source_location\n",
    "                                test_env.agents[i].has_item = 1\n",
    "                                test_env.agents[i].direction = True\n",
    "                        # Reset central clock and counters for safety\n",
    "                        test_env.clock = 0\n",
    "                        test_env.collision_count = 0\n",
    "                        test_env.round_trip_count = 0\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            test_env.agent_states[i] = 0\n",
    "                            test_env.agent_round_trips[i] = False\n",
    "\n",
    "                        # Simulate until first agent (index 0) completes B->A->B or until 25 steps per agent\n",
    "                        agent0_success = False\n",
    "                        collided = False\n",
    "                        max_steps = 25  # max moves per agent\n",
    "                        steps_taken = 0\n",
    "                        for t in range(max_steps * test_env.num_agents):\n",
    "                            aid = test_env.get_next_agent()\n",
    "                            state = test_env.get_state(aid)\n",
    "                            action = greedy_action(state)\n",
    "                            test_env.take_action(aid, action)\n",
    "                            test_env.check_collisions()\n",
    "                            # If any collision occurs, note if agent0 was involved\n",
    "                            if test_env.collision_count > 0:\n",
    "                                for ag in test_env.agents:\n",
    "                                    if ag.collision_penalty != 0:\n",
    "                                        if ag.agent_id == 0:\n",
    "                                            collided = True\n",
    "                                        ag.collision_penalty = 0\n",
    "                            # Check if agent 0 completed B->A->B (round trip)\n",
    "                            # B->A->B is completed when agent0 goes from B (start) -> A (pickup) -> B (drop-off).\n",
    "                            # In our env logic, that corresponds to agent_states[0] reaching 3 (completed) or round_trip_count increment.\n",
    "                            if test_env.agent_round_trips[0]:\n",
    "                                agent0_success = True\n",
    "                                break\n",
    "                            steps_taken += 1\n",
    "                        # Determine success/failure for this test\n",
    "                        if agent0_success and not collided:\n",
    "                            success_count += 1\n",
    "                            stats['successful_tests'] += 1\n",
    "                            stats['distribution_success'][f'{b_agents},{a_agents}'] += 1\n",
    "                            stats['avg_steps_successful'] += steps_taken\n",
    "                        else:\n",
    "                            # Determine failure reason\n",
    "                            if not agent0_success and steps_taken >= max_steps * test_env.num_agents:\n",
    "                                stats['failures']['timeout'] += 1\n",
    "                            elif collided:\n",
    "                                stats['failures']['collisions'] += 1\n",
    "                            else:\n",
    "                                stats['failures']['incomplete'] += 1\n",
    "\n",
    "    # Calculate success rate and average steps\n",
    "    success_rate = success_count / total_tests\n",
    "    if stats['successful_tests'] > 0:\n",
    "        stats['avg_steps_successful'] /= stats['successful_tests']\n",
    "    return success_rate, stats\n",
    "\n",
    "# Run evaluation\n",
    "success_rate, eval_stats = evaluate_model(trained_model)\n",
    "print(f\"Success Rate: {success_rate*100:.2f}%\")\n",
    "print(\"Success rate per distribution:\", {dist: eval_stats['distribution_success'][dist] for dist in eval_stats['distribution_success']})\n",
    "print(\"Failures breakdown:\", eval_stats['failures'])\n",
    "print(f\"Average steps for successful episodes: {eval_stats['avg_steps_successful']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3af05-83d2-40c6-a8ed-5fa605c70d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
