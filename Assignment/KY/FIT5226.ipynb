{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "40d40f18-18f0-4900-8ec0-79f11ae5e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and Agent Definitions\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define agent class to hold state\n",
    "class QTableAgent:\n",
    "    def __init__(self, agent_id):\n",
    "        self.agent_id = agent_id\n",
    "        self.has_item = 0        # 1 if carrying an item, else 0\n",
    "        self.direction = None    # True if heading A->B, False if heading B->A\n",
    "        self.position = None\n",
    "        self.previous_position = None\n",
    "        self.local_mask = 0      # 8-bit mask for opposite-direction neighbors\n",
    "        self.update_order = None # order index for central clock updates\n",
    "        self.collision_penalty = 0\n",
    "        self.completed_round_trip = False\n",
    "\n",
    "# Grid world environment for multi-agent shuttle\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, n=5, m=5, num_agents=4, food_source_location=None, nest_location=None):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.num_agents = num_agents\n",
    "        # Set A (food source) and B (nest) locations\n",
    "        self.food_source_location = food_source_location if food_source_location is not None else (0, 0)\n",
    "        self.nest_location = nest_location if nest_location is not None else (n-1, m-1)\n",
    "        # Initialize agents list\n",
    "        self.agents = [QTableAgent(i) for i in range(num_agents)]\n",
    "        # Define relative positions for the 8 neighboring directions (N, NE, E, SE, S, SW, W, NW)\n",
    "        self.directions = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                            (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        # Central clock for coordinated updates\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(num_agents))  # default round-robin order\n",
    "        # Collision and round-trip tracking\n",
    "        self.collision_count = 0\n",
    "        self.collision_penalty_value = -50  # penalty applied to each agent in a collision\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * num_agents       # progress state (0->3) for each agent's round trip\n",
    "        self.agent_round_trips = [False] * num_agents  # whether each agent completed a round trip\n",
    "        # Place agents at start positions\n",
    "        self._reset()  # randomize initial agent placements at A or B\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset agents to random start at A or B, and reset environment counters.\"\"\"\n",
    "        # Randomize each agent's starting position: 50% at A (with item), 50% at B (empty)\n",
    "        for agent in self.agents:\n",
    "            if random.random() < 0.5:\n",
    "                agent.position = self.food_source_location\n",
    "                agent.has_item = 1\n",
    "                agent.direction = True   # carrying item, so heading toward B\n",
    "            else:\n",
    "                agent.position = self.nest_location\n",
    "                agent.has_item = 0\n",
    "                agent.direction = False  # not carrying, so heading toward A\n",
    "            agent.previous_position = None\n",
    "            agent.local_mask = 0\n",
    "            agent.collision_penalty = 0\n",
    "            agent.completed_round_trip = False\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(self.num_agents))\n",
    "\n",
    "    \n",
    "    def reset_environment_counters(self):\n",
    "        \"\"\"Reset only environment counters, not agent positions\"\"\"\n",
    "        self.collision_count = 0\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * self.num_agents\n",
    "        self.agent_round_trips = [False] * self.num_agents\n",
    "\n",
    "    def get_next_agent(self):\n",
    "        \"\"\"Return the next agent ID to act, based on the central clock (round-robin).\"\"\"\n",
    "        agent_id = self.update_sequence[self.clock % self.num_agents]\n",
    "        self.clock += 1\n",
    "        return agent_id\n",
    "\n",
    "    def get_local_mask(self, agent_id):\n",
    "        \"\"\"\n",
    "        Compute an 8-bit mask for agent agent_id indicating which of the 8 neighboring cells\n",
    "        contain an agent moving in the opposite direction (potential head-on collision threat).\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        x, y = agent.position\n",
    "        mask = 0\n",
    "        # Check all neighbors\n",
    "        for i, (dx, dy) in enumerate(self.directions):\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.n and 0 <= ny < self.m:\n",
    "                for other in self.agents:\n",
    "                    if other.agent_id != agent_id:\n",
    "                        if other.position == (nx, ny) and other.direction != agent.direction:\n",
    "                            # Neighbor cell occupied by an opposite-direction agent\n",
    "                            mask |= (1 << i)\n",
    "        return mask\n",
    "\n",
    "    def get_state(self, agent_id):\n",
    "        \"\"\"Return the 15-dimensional state vector for the given agent.\"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Update the agent's local mask (for visualization/debugging if needed)\n",
    "        agent.local_mask = self.get_local_mask(agent_id)\n",
    "        # Construct state: position (x,y), A coords, B coords, carry flag, and 8 neighbor flags\n",
    "        x, y = agent.position\n",
    "        state = [\n",
    "            x, y,\n",
    "            self.food_source_location[0], self.food_source_location[1],\n",
    "            self.nest_location[0], self.nest_location[1],\n",
    "            agent.has_item\n",
    "        ]\n",
    "        # Append 8 binary flags indicating opposite-direction agents around\n",
    "        for i in range(8):\n",
    "            flag = (agent.local_mask >> i) & 1\n",
    "            state.append(flag)\n",
    "        return state\n",
    "\n",
    "    def check_collisions(self):\n",
    "        \"\"\"\n",
    "        Detect head-on collisions: if any non-endpoint cell is occupied by at least one agent with an item \n",
    "        and at least one without an item (i.e., agents heading in opposite directions). \n",
    "        Apply collision penalty to those agents.\n",
    "        \"\"\"\n",
    "        positions = {}\n",
    "        # Group agents by their current position\n",
    "        for agent in self.agents:\n",
    "            positions.setdefault(agent.position, []).append(agent)\n",
    "        # Check each position that has more than one agent\n",
    "        for pos, agents in positions.items():\n",
    "            if len(agents) < 2:\n",
    "                continue\n",
    "            # Ignore collisions at A or B (agents clustering at endpoints is allowed)\n",
    "            if pos == self.food_source_location or pos == self.nest_location:\n",
    "                continue\n",
    "            # Check if there's at least one carrying and one not carrying agent\n",
    "            carrying = any(a.has_item for a in agents)\n",
    "            not_carrying = any(not a.has_item for a in agents)\n",
    "            if carrying and not_carrying:\n",
    "                # Head-on collision detected\n",
    "                self.collision_count += 1\n",
    "                for agent in agents:\n",
    "                    agent.collision_penalty += self.collision_penalty_value\n",
    "\n",
    "    def take_action(self, agent_id, action):\n",
    "        \"\"\"\n",
    "        Execute the given action for agent agent_id. \n",
    "        Moves the agent, applies pickup/dropoff, updates round-trip state, and returns the reward.\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Save previous position (for potential collision checks or info)\n",
    "        agent.previous_position = agent.position\n",
    "        x, y = agent.position\n",
    "        # Determine next position based on action\n",
    "        if action == 0:   # Up\n",
    "            nx, ny = max(x - 1, 0), y\n",
    "        elif action == 1: # Down\n",
    "            nx, ny = min(x + 1, self.n - 1), y\n",
    "        elif action == 2: # Left\n",
    "            nx, ny = x, max(y - 1, 0)\n",
    "        elif action == 3: # Right\n",
    "            nx, ny = x, min(y + 1, self.m - 1)\n",
    "        else:\n",
    "            nx, ny = x, y  # no-op for safety (should not happen)\n",
    "        next_pos = (nx, ny)\n",
    "        # Default step penalty\n",
    "        reward = -1\n",
    "        # Check for pickup or drop-off events\n",
    "        if next_pos == self.food_source_location and agent.has_item == 0:\n",
    "            # Arriving at A without an item -> pick up item\n",
    "            agent.has_item = 1\n",
    "            reward += 40   # net +39 for pickup (reward was -1, now -1+40)\n",
    "        elif next_pos == self.nest_location and agent.has_item == 1:\n",
    "            # Arriving at B with an item -> drop off\n",
    "            agent.has_item = 0\n",
    "            reward += 60   # net +59 for successful delivery\n",
    "        # Update agent's position\n",
    "        agent.position = next_pos\n",
    "        # Update agent's travel direction based on carry status\n",
    "        agent.direction = True if agent.has_item else False\n",
    "\n",
    "        # Update agent's round-trip progress state machine:\n",
    "        # States: 0 (not started or reset), 1 (halfway), 2 (reached opposite end), 3 (completed round trip).\n",
    "        # A -> B -> A trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 1  # Picked up at A, heading to B\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 2  # Dropped off at B (halfway done A->B->A)\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.food_source_location and agent.has_item == 0:\n",
    "            # Came back to A without item -> completed A->B->A round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        # B -> A -> B trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 1  # Started at B, heading to A\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 2  # Picked up at A, heading back to B\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            # Returned to B with item delivered -> completed B->A->B round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2df5fecf-4c58-4b08-abd1-a22444f17cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent Setup and Replay Buffer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Replay buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            # Remove oldest experience if at capacity\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states      = np.array([exp[0] for exp in batch], dtype=np.float32)\n",
    "        actions     = np.array([exp[1] for exp in batch], dtype=np.int64)\n",
    "        rewards     = np.array([exp[2] for exp in batch], dtype=np.float32)\n",
    "        next_states = np.array([exp[3] for exp in batch], dtype=np.float32)\n",
    "        dones       = np.array([exp[4] for exp in batch], dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize DQN policy network and target network\n",
    "state_dim = 15   # state features (from environment state vector)\n",
    "action_dim = 4   # four possible actions\n",
    "policy_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "target_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "# Copy weights from policy to target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# DQN hyperparameters\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.95             # discount factor\n",
    "batch_size = 64\n",
    "target_update_freq = 250 # how often to sync target net (in updates)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Epsilon-greedy exploration schedule\n",
    "class EpsilonScheduler:\n",
    "    def __init__(self, start=1.0, end=0.1, decay=0.995):\n",
    "        self.epsilon = start\n",
    "        self.min_epsilon = end\n",
    "        self.decay = decay\n",
    "    def get_epsilon(self):\n",
    "        # Return current epsilon and decay it for next call\n",
    "        eps = self.epsilon\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.decay)\n",
    "        return eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "edee3e26-6f8c-4251-8248-00458bdac9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Ep    1- 100 | Epsilon 0.61 | Reward   -79459 | Coll  452 | RT  100 | AvgLoss 369.7001 | Steps 6,649 | CumColl 452 | Time    7.8s\n",
      "Ep  101- 200 | Epsilon 0.37 | Reward   -59224 | Coll  349 | RT   97 | AvgLoss 365.2913 | Steps 12,793 | CumColl 801 | Time   15.2s\n",
      "Ep  201- 300 | Epsilon 0.22 | Reward   -18815 | Coll  127 | RT   99 | AvgLoss 341.0029 | Steps 18,098 | CumColl 928 | Time   21.6s\n",
      "Ep  301- 400 | Epsilon 0.14 | Reward   -11582 | Coll   96 | RT   99 | AvgLoss 309.1443 | Steps 22,930 | CumColl 1,024 | Time   28.2s\n",
      "Ep  401- 500 | Epsilon 0.08 | Reward    -2213 | Coll   49 | RT  100 | AvgLoss 290.6578 | Steps 27,903 | CumColl 1,073 | Time   34.4s\n",
      "Ep  501- 600 | Epsilon 0.05 | Reward     -622 | Coll   49 | RT   98 | AvgLoss 255.7342 | Steps 32,825 | CumColl 1,122 | Time   40.5s\n",
      "Ep  601- 700 | Epsilon 0.05 | Reward     4818 | Coll   32 | RT  100 | AvgLoss 238.6921 | Steps 36,597 | CumColl 1,154 | Time   45.2s\n",
      "Ep  701- 800 | Epsilon 0.05 | Reward     4232 | Coll   31 | RT  100 | AvgLoss 228.8177 | Steps 40,715 | CumColl 1,185 | Time   50.4s\n",
      "Ep  801- 900 | Epsilon 0.05 | Reward     5290 | Coll   39 | RT   99 | AvgLoss 220.7146 | Steps 44,285 | CumColl 1,224 | Time   54.9s\n",
      "Ep  901-1000 | Epsilon 0.05 | Reward     5763 | Coll   45 | RT  100 | AvgLoss 212.4463 | Steps 47,812 | CumColl 1,269 | Time   59.2s\n",
      "Ep 1001-1100 | Epsilon 0.05 | Reward    11616 | Coll   15 | RT  100 | AvgLoss 216.6050 | Steps 50,356 | CumColl 1,284 | Time   62.5s\n",
      "Ep 1101-1200 | Epsilon 0.05 | Reward    12843 | Coll   18 | RT  100 | AvgLoss 205.4334 | Steps 53,203 | CumColl 1,302 | Time   66.1s\n",
      "Ep 1201-1300 | Epsilon 0.05 | Reward     8418 | Coll   41 | RT  100 | AvgLoss 189.8960 | Steps 55,935 | CumColl 1,343 | Time   69.5s\n",
      "Ep 1301-1400 | Epsilon 0.05 | Reward    12096 | Coll   25 | RT  100 | AvgLoss 180.2611 | Steps 58,339 | CumColl 1,368 | Time   72.6s\n",
      "Ep 1401-1500 | Epsilon 0.05 | Reward    11111 | Coll   24 | RT   99 | AvgLoss 165.0557 | Steps 61,198 | CumColl 1,392 | Time   76.2s\n",
      "Ep 1501-1600 | Epsilon 0.05 | Reward    13067 | Coll   21 | RT  100 | AvgLoss 151.3808 | Steps 63,611 | CumColl 1,413 | Time   79.3s\n",
      "Ep 1601-1700 | Epsilon 0.05 | Reward    13522 | Coll   23 | RT  100 | AvgLoss 149.7311 | Steps 65,939 | CumColl 1,436 | Time   82.2s\n",
      "Ep 1701-1800 | Epsilon 0.05 | Reward     8811 | Coll   41 | RT  100 | AvgLoss 148.3897 | Steps 68,118 | CumColl 1,477 | Time   85.3s\n",
      "Ep 1801-1900 | Epsilon 0.05 | Reward     6197 | Coll   61 | RT  100 | AvgLoss 157.1600 | Steps 70,531 | CumColl 1,538 | Time   88.3s\n",
      "Ep 1901-2000 | Epsilon 0.05 | Reward    12377 | Coll   32 | RT  100 | AvgLoss 154.7114 | Steps 72,914 | CumColl 1,570 | Time   91.4s\n",
      "Ep 2001-2100 | Epsilon 0.05 | Reward    12319 | Coll   27 | RT  100 | AvgLoss 149.1386 | Steps 75,255 | CumColl 1,597 | Time   94.4s\n",
      "Ep 2101-2200 | Epsilon 0.05 | Reward    11787 | Coll   28 | RT  100 | AvgLoss 149.6448 | Steps 77,498 | CumColl 1,625 | Time   97.2s\n",
      "Ep 2201-2300 | Epsilon 0.05 | Reward    10597 | Coll   52 | RT  100 | AvgLoss 155.4955 | Steps 80,091 | CumColl 1,677 | Time  100.4s\n",
      "Ep 2301-2400 | Epsilon 0.05 | Reward    12450 | Coll   38 | RT  100 | AvgLoss 162.7749 | Steps 82,491 | CumColl 1,715 | Time  103.5s\n",
      "Ep 2401-2500 | Epsilon 0.05 | Reward    10926 | Coll   44 | RT  100 | AvgLoss 163.6394 | Steps 84,875 | CumColl 1,759 | Time  106.4s\n",
      "Ep 2501-2600 | Epsilon 0.05 | Reward    14381 | Coll   33 | RT  100 | AvgLoss 169.9889 | Steps 87,304 | CumColl 1,792 | Time  109.5s\n",
      "Ep 2601-2700 | Epsilon 0.05 | Reward     9487 | Coll   50 | RT  100 | AvgLoss 164.5743 | Steps 89,967 | CumColl 1,842 | Time  112.9s\n",
      "Ep 2701-2800 | Epsilon 0.05 | Reward    11303 | Coll   50 | RT  100 | AvgLoss 173.7079 | Steps 92,464 | CumColl 1,892 | Time  116.0s\n",
      "Ep 2801-2900 | Epsilon 0.05 | Reward     8421 | Coll   65 | RT  100 | AvgLoss 178.5620 | Steps 94,923 | CumColl 1,957 | Time  119.1s\n",
      "Ep 2901-3000 | Epsilon 0.05 | Reward     7620 | Coll   56 | RT  100 | AvgLoss 177.5346 | Steps 97,223 | CumColl 2,013 | Time  122.0s\n",
      "Ep 3001-3100 | Epsilon 0.05 | Reward    10672 | Coll   50 | RT  100 | AvgLoss 184.6637 | Steps 99,551 | CumColl 2,063 | Time  125.2s\n",
      "Ep 3101-3200 | Epsilon 0.05 | Reward     7508 | Coll   65 | RT  100 | AvgLoss 186.3856 | Steps 101,943 | CumColl 2,128 | Time  128.2s\n",
      "Ep 3201-3300 | Epsilon 0.05 | Reward    10328 | Coll   66 | RT  100 | AvgLoss 193.2569 | Steps 104,395 | CumColl 2,194 | Time  131.2s\n",
      "Ep 3301-3400 | Epsilon 0.05 | Reward     9831 | Coll   64 | RT  100 | AvgLoss 193.9249 | Steps 106,614 | CumColl 2,258 | Time  134.0s\n",
      "Ep 3401-3500 | Epsilon 0.05 | Reward     9436 | Coll   62 | RT  100 | AvgLoss 193.6968 | Steps 108,838 | CumColl 2,320 | Time  136.8s\n",
      "Ep 3501-3600 | Epsilon 0.05 | Reward     5829 | Coll   86 | RT  100 | AvgLoss 196.1578 | Steps 111,149 | CumColl 2,406 | Time  139.8s\n",
      "Ep 3601-3700 | Epsilon 0.05 | Reward     9634 | Coll   66 | RT  100 | AvgLoss 202.9561 | Steps 113,365 | CumColl 2,472 | Time  142.6s\n",
      "Ep 3701-3800 | Epsilon 0.05 | Reward     2534 | Coll  103 | RT  100 | AvgLoss 214.6881 | Steps 115,641 | CumColl 2,575 | Time  145.5s\n",
      "Ep 3801-3900 | Epsilon 0.05 | Reward     5591 | Coll   86 | RT  100 | AvgLoss 221.4963 | Steps 117,790 | CumColl 2,661 | Time  148.3s\n",
      "Ep 3901-4000 | Epsilon 0.05 | Reward     3378 | Coll  100 | RT  100 | AvgLoss 227.8364 | Steps 120,262 | CumColl 2,761 | Time  151.6s\n",
      "Ep 4001-4100 | Epsilon 0.05 | Reward    11647 | Coll   55 | RT  100 | AvgLoss 221.4649 | Steps 122,675 | CumColl 2,816 | Time  154.6s\n",
      "Ep 4101-4200 | Epsilon 0.05 | Reward    10140 | Coll   56 | RT   99 | AvgLoss 230.5184 | Steps 125,335 | CumColl 2,872 | Time  158.0s\n",
      "Ep 4201-4300 | Epsilon 0.05 | Reward    10721 | Coll   64 | RT  100 | AvgLoss 226.5941 | Steps 127,654 | CumColl 2,936 | Time  161.0s\n",
      "Ep 4301-4400 | Epsilon 0.05 | Reward    11692 | Coll   56 | RT  100 | AvgLoss 230.4253 | Steps 130,062 | CumColl 2,992 | Time  164.1s\n",
      "Ep 4401-4500 | Epsilon 0.05 | Reward     7583 | Coll   76 | RT  100 | AvgLoss 235.3120 | Steps 132,439 | CumColl 3,068 | Time  167.1s\n",
      "Ep 4501-4600 | Epsilon 0.05 | Reward     5911 | Coll   81 | RT  100 | AvgLoss 238.6966 | Steps 134,688 | CumColl 3,149 | Time  169.9s\n",
      "Ep 4601-4700 | Epsilon 0.05 | Reward    13488 | Coll   55 | RT  100 | AvgLoss 252.6042 | Steps 137,040 | CumColl 3,204 | Time  172.9s\n",
      "Ep 4701-4800 | Epsilon 0.05 | Reward     9342 | Coll   63 | RT  100 | AvgLoss 249.5869 | Steps 139,318 | CumColl 3,267 | Time  175.8s\n",
      "Ep 4801-4900 | Epsilon 0.05 | Reward    17344 | Coll   31 | RT  100 | AvgLoss 248.0123 | Steps 141,844 | CumColl 3,298 | Time  179.0s\n",
      "Ep 4901-5000 | Epsilon 0.05 | Reward    12206 | Coll   57 | RT  100 | AvgLoss 248.4067 | Steps 143,998 | CumColl 3,355 | Time  181.7s\n",
      "Ep 5001-5100 | Epsilon 0.05 | Reward    15379 | Coll   44 | RT  100 | AvgLoss 248.7140 | Steps 146,489 | CumColl 3,399 | Time  184.8s\n",
      "Ep 5101-5200 | Epsilon 0.05 | Reward     9895 | Coll   63 | RT  100 | AvgLoss 239.2571 | Steps 148,634 | CumColl 3,462 | Time  187.5s\n",
      "Ep 5201-5300 | Epsilon 0.05 | Reward    15167 | Coll   39 | RT  100 | AvgLoss 253.0803 | Steps 151,247 | CumColl 3,501 | Time  191.0s\n",
      "Ep 5301-5400 | Epsilon 0.05 | Reward    10374 | Coll   64 | RT  100 | AvgLoss 248.9191 | Steps 153,883 | CumColl 3,565 | Time  194.3s\n",
      "Ep 5401-5500 | Epsilon 0.05 | Reward    15835 | Coll   42 | RT  100 | AvgLoss 235.5515 | Steps 156,488 | CumColl 3,607 | Time  197.6s\n",
      "Ep 5501-5600 | Epsilon 0.05 | Reward    13131 | Coll   49 | RT  100 | AvgLoss 237.9077 | Steps 158,767 | CumColl 3,656 | Time  200.5s\n",
      "Ep 5601-5700 | Epsilon 0.05 | Reward    13337 | Coll   51 | RT  100 | AvgLoss 227.7515 | Steps 161,040 | CumColl 3,707 | Time  203.5s\n",
      "Ep 5701-5800 | Epsilon 0.05 | Reward    12671 | Coll   40 | RT  100 | AvgLoss 226.4354 | Steps 163,249 | CumColl 3,747 | Time  206.3s\n",
      "Ep 5801-5900 | Epsilon 0.05 | Reward    15180 | Coll   32 | RT  100 | AvgLoss 223.5576 | Steps 165,519 | CumColl 3,779 | Time  209.2s\n",
      "Ep 5901-6000 | Epsilon 0.05 | Reward    17819 | Coll   31 | RT  100 | AvgLoss 213.6857 | Steps 167,850 | CumColl 3,810 | Time  212.2s\n",
      "Ep 6001-6100 | Epsilon 0.05 | Reward     8708 | Coll   68 | RT  100 | AvgLoss 204.4200 | Steps 170,002 | CumColl 3,878 | Time  214.9s\n",
      "Ep 6101-6200 | Epsilon 0.05 | Reward    19604 | Coll   27 | RT  100 | AvgLoss 204.8586 | Steps 172,688 | CumColl 3,905 | Time  218.3s\n",
      "Ep 6201-6300 | Epsilon 0.05 | Reward    16222 | Coll   34 | RT  100 | AvgLoss 193.1319 | Steps 175,066 | CumColl 3,939 | Time  221.4s\n",
      "Ep 6301-6400 | Epsilon 0.05 | Reward    15265 | Coll   44 | RT  100 | AvgLoss 196.4460 | Steps 177,411 | CumColl 3,983 | Time  224.4s\n",
      "Stopping training early due to collision budget limit reached.\n",
      "Training completed.\n",
      "Average Reward: 70.32, Average Collisions: 0.62, Average RoundTrips: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Logging utility to record training metrics\n",
    "class MetricLogger:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.collisions = []\n",
    "        self.round_trips = []\n",
    "        self.lengths = []\n",
    "        self.episodes = []\n",
    "    def log_episode(self, total_reward, collisions, round_trips, length):\n",
    "        self.rewards.append(total_reward)\n",
    "        self.collisions.append(collisions)\n",
    "        self.round_trips.append(round_trips)\n",
    "        self.lengths.append(length)\n",
    "        self.episodes.append(len(self.episodes) + 1)\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"mean_reward\": np.mean(self.rewards) if self.rewards else 0.0,\n",
    "            \"mean_collisions\": np.mean(self.collisions) if self.collisions else 0.0,\n",
    "            \"mean_round_trips\": np.mean(self.round_trips) if self.round_trips else 0.0,\n",
    "            \"mean_length\": np.mean(self.lengths) if self.lengths else 0.0\n",
    "        }\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training metrics for every episode\"\"\"\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            \n",
    "        # Plot rewards\n",
    "        axs[0, 0].plot(self.episodes, self.rewards, '-', alpha=0.7)\n",
    "        axs[0, 0].set_title('Total Reward per Episode')\n",
    "        axs[0, 0].set_xlabel('Episode')\n",
    "        axs[0, 0].set_ylabel('Reward')\n",
    "        axs[0, 0].grid(True)\n",
    "        \n",
    "        # Plot collisions\n",
    "        axs[0, 1].plot(self.episodes, self.collisions, '-', alpha=0.7)\n",
    "        axs[0, 1].set_title('Collisions per Episode')\n",
    "        axs[0, 1].set_xlabel('Episode')\n",
    "        axs[0, 1].set_ylabel('Number of Collisions')\n",
    "        axs[0, 1].grid(True)\n",
    "        \n",
    "        # Plot round trips\n",
    "        axs[1, 0].plot(self.episodes, self.round_trips, '-', alpha=0.7)\n",
    "        axs[1, 0].set_title('Round Trips per Episode')\n",
    "        axs[1, 0].set_xlabel('Episode')\n",
    "        axs[1, 0].set_ylabel('Number of Round Trips')\n",
    "        axs[1, 0].grid(True)\n",
    "        \n",
    "        # Plot episode lengths\n",
    "        axs[1, 1].plot(self.episodes, self.lengths, '-', alpha=0.7)\n",
    "        axs[1, 1].set_title('Episode Length')\n",
    "        axs[1, 1].set_xlabel('Episode')\n",
    "        axs[1, 1].set_ylabel('Steps')\n",
    "        axs[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.close()\n",
    "\n",
    "# Training parameters and budgets\n",
    "num_episodes = 8000\n",
    "max_steps_per_episode = 120   # safety cap on steps per episode (will often terminate earlier)\n",
    "buffer_capacity = 50000\n",
    "step_budget = 1_500_000       # max training steps (across all agents)\n",
    "collision_budget = 4000       # max total collisions during training\n",
    "walltime_budget = 600         # max wall-clock time (seconds) for training\n",
    "\n",
    "# Initialize environment, replay buffer, epsilon scheduler, and metric logger\n",
    "env = GridWorldEnvironment()\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "eps_scheduler = EpsilonScheduler(start=1.0, end=0.05, decay=0.995)\n",
    "logger = MetricLogger()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "# Training loop\n",
    "import time\n",
    "start_time = time.time()\n",
    "total_steps = 0\n",
    "total_collisions = 0\n",
    "\n",
    "# ------------ 100â€‘episode window accumulators ------------\n",
    "window_reward_sum     = 0.0\n",
    "window_coll_sum       = 0\n",
    "window_roundtrip_sum  = 0\n",
    "window_loss_sum       = 0.0\n",
    "window_loss_count     = 0\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    while True:\n",
    "        a_row = np.random.randint(env.n)\n",
    "        a_col = np.random.randint(env.m)\n",
    "        b_row = np.random.randint(env.n)\n",
    "        b_col = np.random.randint(env.m)\n",
    "        if (a_row, a_col) != (b_row, b_col):\n",
    "            break\n",
    "    env.food_source_location = (a_row, a_col)\n",
    "    env.nest_location = (b_row, b_col)\n",
    "    env.reset_environment_counters()\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_collisions = 0\n",
    "    episode_round_trips = 0\n",
    "    eps = eps_scheduler.get_epsilon()\n",
    "    done_episode = False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        for _ in range(env.num_agents):\n",
    "            agent_id = env.get_next_agent()\n",
    "            state = env.get_state(agent_id)\n",
    "\n",
    "            if random.random() < eps:\n",
    "                action = random.randrange(action_dim)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = int(torch.argmax(q_values).item())\n",
    "\n",
    "            reward = env.take_action(agent_id, action)\n",
    "            env.check_collisions()\n",
    "\n",
    "            if env.collision_count > episode_collisions:\n",
    "                new_collisions = env.collision_count - episode_collisions\n",
    "                total_collisions += new_collisions\n",
    "                episode_collisions = env.collision_count\n",
    "                for ag in env.agents:\n",
    "                    if ag.collision_penalty != 0:\n",
    "                        if ag.agent_id != agent_id:\n",
    "                            reward += ag.collision_penalty\n",
    "                        episode_reward += ag.collision_penalty\n",
    "                        ag.collision_penalty = 0\n",
    "\n",
    "            next_state = env.get_state(agent_id)\n",
    "            done = False\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                state_batch = torch.tensor(states, dtype=torch.float32)\n",
    "                next_state_batch = torch.tensor(next_states, dtype=torch.float32)\n",
    "                action_batch = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "                reward_batch = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "                done_batch = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                q_current = policy_net(state_batch).gather(1, action_batch)\n",
    "                q_next = target_net(next_state_batch).max(dim=1, keepdim=True)[0]\n",
    "                q_target = reward_batch + gamma * q_next * (1 - done_batch)\n",
    "\n",
    "                loss = loss_fn(q_current, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track loss in 100-episode window\n",
    "                window_loss_sum += loss.item()\n",
    "                window_loss_count += 1\n",
    "\n",
    "            total_steps += 1\n",
    "            if env.round_trip_count > 0:\n",
    "                done_episode = True\n",
    "            if total_steps >= step_budget or total_collisions >= collision_budget or (time.time() - start_time) >= walltime_budget:\n",
    "                done_episode = True\n",
    "            if done_episode:\n",
    "                break\n",
    "\n",
    "        if total_steps % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done_episode or step == max_steps_per_episode - 1:\n",
    "            episode_round_trips = env.round_trip_count\n",
    "            break\n",
    "\n",
    "    logger.log_episode(episode_reward, episode_collisions, episode_round_trips, step+1)\n",
    "\n",
    "    # --- Accumulate episode stats for current 100-episode window ---\n",
    "    window_reward_sum    += episode_reward\n",
    "    window_coll_sum      += episode_collisions\n",
    "    window_roundtrip_sum += episode_round_trips\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    # --- Print block every 100 episodes ---\n",
    "    if episode % 100 == 0:\n",
    "        avg_loss = (window_loss_sum / window_loss_count) if window_loss_count else float('nan')\n",
    "        print(f\"Ep {episode-99:4d}-{episode:4d} | \"\n",
    "              f\"Epsilon {eps:.2f} | \"\n",
    "              f\"Reward {window_reward_sum:8.0f} | \"\n",
    "              f\"Coll {window_coll_sum:4d} | \"\n",
    "              f\"RT {window_roundtrip_sum:4d} | \"\n",
    "              f\"AvgLoss {avg_loss:7.4f} | \"\n",
    "              f\"Steps {total_steps:,} | CumColl {total_collisions:,} | \"\n",
    "              f\"Time {time.time()-start_time:6.1f}s\") \n",
    "        # Reset accumulators\n",
    "        window_reward_sum     = 0.0 #\n",
    "        window_coll_sum       = 0\n",
    "        window_roundtrip_sum  = 0\n",
    "        window_loss_sum       = 0.0\n",
    "        window_loss_count     = 0\n",
    "\n",
    "    # Check stop conditions\n",
    "    if total_steps >= step_budget:\n",
    "        print(\"Stopping training early due to step budget limit reached.\")\n",
    "        break\n",
    "    elif total_collisions >= collision_budget:\n",
    "        print (\"Stopping training early due to collision budget limit reached.\")\n",
    "        break\n",
    "    elif (time.time() - start_time) >= walltime_budget:\n",
    "        print (\"Stopping training early due to time budget limit reached.\")\n",
    "        break\n",
    "\n",
    "# Save trained model\n",
    "torch.save(policy_net.state_dict(), \"trained_model.pth\")\n",
    "print(\"Training completed.\")\n",
    "stats = logger.get_stats()\n",
    "print(f\"Average Reward: {stats['mean_reward']:.2f}, \"\n",
    "      f\"Average Collisions: {stats['mean_collisions']:.2f}, \"\n",
    "      f\"Average RoundTrips: {stats['mean_round_trips']:.2f}\")\n",
    "\n",
    "# add visualization\n",
    "logger.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "15c5b962-72a4-458c-8bee-b383e0278f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 94.33%\n",
      "Success rate per distribution: {'1,3': 562, '2,2': 562, '3,1': 562, '4,0': 578}\n",
      "Failures breakdown: {'collisions': 63, 'timeout': 73, 'incomplete': 0}\n",
      "Average steps for successful episodes: 22.96\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of Trained Policy\n",
    "\n",
    "# Load the trained model for evaluation\n",
    "trained_model = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "trained_model.load_state_dict(torch.load(\"trained_model.pth\", map_location=\"cpu\"))\n",
    "trained_model.eval()\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"Evaluate the trained model on all possible A-B configurations and specified agent distributions.\"\"\"\n",
    "    GRID_SIZE = 5\n",
    "    distributions = [(1,3), (2,2), (3,1), (4,0)]  # (agents at B, agents at A)\n",
    "    success_count = 0\n",
    "    total_tests = 0\n",
    "    stats = {\n",
    "        'total_tests': 0,\n",
    "        'successful_tests': 0,\n",
    "        'failures': {'collisions': 0, 'timeout': 0, 'incomplete': 0},\n",
    "        'distribution_success': {'1,3': 0, '2,2': 0, '3,1': 0, '4,0': 0},\n",
    "        'avg_steps_successful': 0\n",
    "    }\n",
    "    # Helper: choose greedy action using the trained model\n",
    "    def greedy_action(state):\n",
    "        state_t = torch.tensor(state, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            q_vals = model(state_t)\n",
    "        return int(torch.argmax(q_vals).item())\n",
    "\n",
    "    # Iterate over all possible A, B placements\n",
    "    for ax in range(GRID_SIZE):\n",
    "        for ay in range(GRID_SIZE):\n",
    "            for bx in range(GRID_SIZE):\n",
    "                for by in range(GRID_SIZE):\n",
    "                    if (ax, ay) == (bx, by):\n",
    "                        continue  # skip invalid configs where A == B\n",
    "                    for (b_agents, a_agents) in distributions:\n",
    "                        total_tests += 1\n",
    "                        stats['total_tests'] = total_tests\n",
    "                        # Initialize env with given A and B\n",
    "                        test_env = GridWorldEnvironment(n=GRID_SIZE, m=GRID_SIZE, num_agents=4,\n",
    "                                                        food_source_location=(ax, ay),\n",
    "                                                        nest_location=(bx, by))\n",
    "                        # Manually set agents at B or A according to the distribution\n",
    "                        # First agent (index 0) at B, since we test that agent's ability to do B->A->B\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            if i < b_agents:\n",
    "                                # place this agent at B (nest), without item\n",
    "                                test_env.agents[i].position = test_env.nest_location\n",
    "                                test_env.agents[i].has_item = 0\n",
    "                                test_env.agents[i].direction = False\n",
    "                            else:\n",
    "                                # place this agent at A (food source), with item\n",
    "                                test_env.agents[i].position = test_env.food_source_location\n",
    "                                test_env.agents[i].has_item = 1\n",
    "                                test_env.agents[i].direction = True\n",
    "                        # Reset central clock and counters for safety\n",
    "                        test_env.clock = 0\n",
    "                        test_env.collision_count = 0\n",
    "                        test_env.round_trip_count = 0\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            test_env.agent_states[i] = 0\n",
    "                            test_env.agent_round_trips[i] = False\n",
    "\n",
    "                        # Simulate until first agent (index 0) completes B->A->B or until 25 steps per agent\n",
    "                        agent0_success = False\n",
    "                        collided = False\n",
    "                        max_steps = 25  # max moves per agent\n",
    "                        steps_taken = 0\n",
    "                        for t in range(max_steps * test_env.num_agents):\n",
    "                            aid = test_env.get_next_agent()\n",
    "                            state = test_env.get_state(aid)\n",
    "                            action = greedy_action(state)\n",
    "                            test_env.take_action(aid, action)\n",
    "                            test_env.check_collisions()\n",
    "                            # If any collision occurs, note if agent0 was involved\n",
    "                            if test_env.collision_count > 0:\n",
    "                                for ag in test_env.agents:\n",
    "                                    if ag.collision_penalty != 0:\n",
    "                                        if ag.agent_id == 0:\n",
    "                                            collided = True\n",
    "                                        ag.collision_penalty = 0\n",
    "                            # Check if agent 0 completed B->A->B (round trip)\n",
    "                            # B->A->B is completed when agent0 goes from B (start) -> A (pickup) -> B (drop-off).\n",
    "                            # In our env logic, that corresponds to agent_states[0] reaching 3 (completed) or round_trip_count increment.\n",
    "                            if test_env.agent_round_trips[0]:\n",
    "                                agent0_success = True\n",
    "                                break\n",
    "                            steps_taken += 1\n",
    "                        # Determine success/failure for this test\n",
    "                        if agent0_success and not collided:\n",
    "                            success_count += 1\n",
    "                            stats['successful_tests'] += 1\n",
    "                            stats['distribution_success'][f'{b_agents},{a_agents}'] += 1\n",
    "                            stats['avg_steps_successful'] += steps_taken\n",
    "                        else:\n",
    "                            # Determine failure reason\n",
    "                            if not agent0_success and steps_taken >= max_steps * test_env.num_agents:\n",
    "                                stats['failures']['timeout'] += 1\n",
    "                            elif collided:\n",
    "                                stats['failures']['collisions'] += 1\n",
    "                            else:\n",
    "                                stats['failures']['incomplete'] += 1\n",
    "\n",
    "    # Calculate success rate and average steps\n",
    "    success_rate = success_count / total_tests\n",
    "    if stats['successful_tests'] > 0:\n",
    "        stats['avg_steps_successful'] /= stats['successful_tests']\n",
    "    return success_rate, stats\n",
    "\n",
    "# Run evaluation\n",
    "success_rate, eval_stats = evaluate_model(trained_model)\n",
    "print(f\"Success Rate: {success_rate*100:.2f}%\")\n",
    "print(\"Success rate per distribution:\", {dist: eval_stats['distribution_success'][dist] for dist in eval_stats['distribution_success']})\n",
    "print(\"Failures breakdown:\", eval_stats['failures'])\n",
    "print(f\"Average steps for successful episodes: {eval_stats['avg_steps_successful']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3af05-83d2-40c6-a8ed-5fa605c70d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
