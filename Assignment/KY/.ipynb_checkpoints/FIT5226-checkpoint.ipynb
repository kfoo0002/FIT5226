{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d40f18-18f0-4900-8ec0-79f11ae5e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and Agent Definitions\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define agent class to hold state\n",
    "class QTableAgent:\n",
    "    def __init__(self, agent_id):\n",
    "        self.agent_id = agent_id\n",
    "        self.has_item = 0        # 1 if carrying an item, else 0\n",
    "        self.direction = None    # True if heading A->B, False if heading B->A\n",
    "        self.position = None\n",
    "        self.previous_position = None\n",
    "        self.local_mask = 0      # 8-bit mask for opposite-direction neighbors\n",
    "        self.update_order = None # order index for central clock updates\n",
    "        self.collision_penalty = 0\n",
    "        self.completed_round_trip = False\n",
    "\n",
    "# Grid world environment for multi-agent shuttle\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, n=5, m=5, num_agents=4, food_source_location=None, nest_location=None):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.num_agents = num_agents\n",
    "        # Set A (food source) and B (nest) locations\n",
    "        self.food_source_location = food_source_location if food_source_location is not None else (0, 0)\n",
    "        self.nest_location = nest_location if nest_location is not None else (n-1, m-1)\n",
    "        # Initialize agents list\n",
    "        self.agents = [QTableAgent(i) for i in range(num_agents)]\n",
    "        # Define relative positions for the 8 neighboring directions (N, NE, E, SE, S, SW, W, NW)\n",
    "        self.directions = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                            (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        # Central clock for coordinated updates\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(num_agents))  # default round-robin order\n",
    "        # Collision and round-trip tracking\n",
    "        self.collision_count = 0\n",
    "        self.collision_penalty_value = -50  # penalty applied to each agent in a collision\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * num_agents       # progress state (0->3) for each agent's round trip\n",
    "        self.agent_round_trips = [False] * num_agents  # whether each agent completed a round trip\n",
    "        # Place agents at start positions\n",
    "        self._reset()  # randomize initial agent placements at A or B\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset agents to random start at A or B, and reset environment counters.\"\"\"\n",
    "        # Randomize each agent's starting position: 50% at A (with item), 50% at B (empty)\n",
    "        for agent in self.agents:\n",
    "            if random.random() < 0.5:\n",
    "                agent.position = self.food_source_location\n",
    "                agent.has_item = 1\n",
    "                agent.direction = True   # carrying item, so heading toward B\n",
    "            else:\n",
    "                agent.position = self.nest_location\n",
    "                agent.has_item = 0\n",
    "                agent.direction = False  # not carrying, so heading toward A\n",
    "            agent.previous_position = None\n",
    "            agent.local_mask = 0\n",
    "            agent.collision_penalty = 0\n",
    "            agent.completed_round_trip = False\n",
    "        # Reset global counters\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(self.num_agents))  # round-robin order\n",
    "        self.collision_count = 0\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * self.num_agents\n",
    "        self.agent_round_trips = [False] * self.num_agents\n",
    "\n",
    "    def get_next_agent(self):\n",
    "        \"\"\"Return the next agent ID to act, based on the central clock (round-robin).\"\"\"\n",
    "        agent_id = self.update_sequence[self.clock % self.num_agents]\n",
    "        self.clock += 1\n",
    "        return agent_id\n",
    "\n",
    "    def get_local_mask(self, agent_id):\n",
    "        \"\"\"\n",
    "        Compute an 8-bit mask for agent agent_id indicating which of the 8 neighboring cells\n",
    "        contain an agent moving in the opposite direction (potential head-on collision threat).\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        x, y = agent.position\n",
    "        mask = 0\n",
    "        # Check all neighbors\n",
    "        for i, (dx, dy) in enumerate(self.directions):\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.n and 0 <= ny < self.m:\n",
    "                for other in self.agents:\n",
    "                    if other.agent_id != agent_id:\n",
    "                        if other.position == (nx, ny) and other.direction != agent.direction:\n",
    "                            # Neighbor cell occupied by an opposite-direction agent\n",
    "                            mask |= (1 << i)\n",
    "        return mask\n",
    "\n",
    "    def get_state(self, agent_id):\n",
    "        \"\"\"Return the 15-dimensional state vector for the given agent.\"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Update the agent's local mask (for visualization/debugging if needed)\n",
    "        agent.local_mask = self.get_local_mask(agent_id)\n",
    "        # Construct state: position (x,y), A coords, B coords, carry flag, and 8 neighbor flags\n",
    "        x, y = agent.position\n",
    "        state = [\n",
    "            x, y,\n",
    "            self.food_source_location[0], self.food_source_location[1],\n",
    "            self.nest_location[0], self.nest_location[1],\n",
    "            agent.has_item\n",
    "        ]\n",
    "        # Append 8 binary flags indicating opposite-direction agents around\n",
    "        for i in range(8):\n",
    "            flag = (agent.local_mask >> i) & 1\n",
    "            state.append(flag)\n",
    "        return state\n",
    "\n",
    "    def check_collisions(self):\n",
    "        \"\"\"\n",
    "        Detect head-on collisions: if any non-endpoint cell is occupied by at least one agent with an item \n",
    "        and at least one without an item (i.e., agents heading in opposite directions). \n",
    "        Apply collision penalty to those agents.\n",
    "        \"\"\"\n",
    "        positions = {}\n",
    "        # Group agents by their current position\n",
    "        for agent in self.agents:\n",
    "            positions.setdefault(agent.position, []).append(agent)\n",
    "        # Check each position that has more than one agent\n",
    "        for pos, agents in positions.items():\n",
    "            if len(agents) < 2:\n",
    "                continue\n",
    "            # Ignore collisions at A or B (agents clustering at endpoints is allowed)\n",
    "            if pos == self.food_source_location or pos == self.nest_location:\n",
    "                continue\n",
    "            # Check if there's at least one carrying and one not carrying agent\n",
    "            carrying = any(a.has_item for a in agents)\n",
    "            not_carrying = any(not a.has_item for a in agents)\n",
    "            if carrying and not_carrying:\n",
    "                # Head-on collision detected\n",
    "                self.collision_count += 1\n",
    "                for agent in agents:\n",
    "                    agent.collision_penalty += self.collision_penalty_value\n",
    "\n",
    "    def take_action(self, agent_id, action):\n",
    "        \"\"\"\n",
    "        Execute the given action for agent agent_id. \n",
    "        Moves the agent, applies pickup/dropoff, updates round-trip state, and returns the reward.\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Save previous position (for potential collision checks or info)\n",
    "        agent.previous_position = agent.position\n",
    "        x, y = agent.position\n",
    "        # Determine next position based on action\n",
    "        if action == 0:   # Up\n",
    "            nx, ny = max(x - 1, 0), y\n",
    "        elif action == 1: # Down\n",
    "            nx, ny = min(x + 1, self.n - 1), y\n",
    "        elif action == 2: # Left\n",
    "            nx, ny = x, max(y - 1, 0)\n",
    "        elif action == 3: # Right\n",
    "            nx, ny = x, min(y + 1, self.m - 1)\n",
    "        else:\n",
    "            nx, ny = x, y  # no-op for safety (should not happen)\n",
    "        next_pos = (nx, ny)\n",
    "        # Default step penalty\n",
    "        reward = -1\n",
    "        # Check for pickup or drop-off events\n",
    "        if next_pos == self.food_source_location and agent.has_item == 0:\n",
    "            # Arriving at A without an item -> pick up item\n",
    "            agent.has_item = 1\n",
    "            reward += 40   # net +39 for pickup (reward was -1, now -1+40)\n",
    "        elif next_pos == self.nest_location and agent.has_item == 1:\n",
    "            # Arriving at B with an item -> drop off\n",
    "            agent.has_item = 0\n",
    "            reward += 60   # net +59 for successful delivery\n",
    "        # Update agent's position\n",
    "        agent.position = next_pos\n",
    "        # Update agent's travel direction based on carry status\n",
    "        agent.direction = True if agent.has_item else False\n",
    "\n",
    "        # Update agent's round-trip progress state machine:\n",
    "        # States: 0 (not started or reset), 1 (halfway), 2 (reached opposite end), 3 (completed round trip).\n",
    "        # A -> B -> A trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 1  # Picked up at A, heading to B\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 2  # Dropped off at B (halfway done A->B->A)\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.food_source_location and agent.has_item == 0:\n",
    "            # Came back to A without item -> completed A->B->A round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        # B -> A -> B trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 1  # Started at B, heading to A\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 2  # Picked up at A, heading back to B\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            # Returned to B with item delivered -> completed B->A->B round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5fecf-4c58-4b08-abd1-a22444f17cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent Setup and Replay Buffer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Replay buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            # Remove oldest experience if at capacity\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states      = np.array([exp[0] for exp in batch], dtype=np.float32)\n",
    "        actions     = np.array([exp[1] for exp in batch], dtype=np.int64)\n",
    "        rewards     = np.array([exp[2] for exp in batch], dtype=np.float32)\n",
    "        next_states = np.array([exp[3] for exp in batch], dtype=np.float32)\n",
    "        dones       = np.array([exp[4] for exp in batch], dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize DQN policy network and target network\n",
    "state_dim = 15   # state features (from environment state vector)\n",
    "action_dim = 4   # four possible actions\n",
    "policy_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "target_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "# Copy weights from policy to target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# DQN hyperparameters\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.95             # discount factor\n",
    "batch_size = 64\n",
    "target_update_freq = 250 # how often to sync target net (in updates)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Epsilon-greedy exploration schedule\n",
    "class EpsilonScheduler:\n",
    "    def __init__(self, start=1.0, end=0.1, decay=0.995):\n",
    "        self.epsilon = start\n",
    "        self.min_epsilon = end\n",
    "        self.decay = decay\n",
    "    def get_epsilon(self):\n",
    "        # Return current epsilon and decay it for next call\n",
    "        eps = self.epsilon\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.decay)\n",
    "        return eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee3e26-6f8c-4251-8248-00458bdac9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Logging utility to record training metrics\n",
    "class MetricLogger:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.collisions = []\n",
    "        self.round_trips = []\n",
    "        self.lengths = []\n",
    "    def log_episode(self, total_reward, collisions, round_trips, length):\n",
    "        self.rewards.append(total_reward)\n",
    "        self.collisions.append(collisions)\n",
    "        self.round_trips.append(round_trips)\n",
    "        self.lengths.append(length)\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"mean_reward\": np.mean(self.rewards) if self.rewards else 0.0,\n",
    "            \"mean_collisions\": np.mean(self.collisions) if self.collisions else 0.0,\n",
    "            \"mean_round_trips\": np.mean(self.round_trips) if self.round_trips else 0.0,\n",
    "            \"mean_length\": np.mean(self.lengths) if self.lengths else 0.0\n",
    "        }\n",
    "\n",
    "# Training parameters and budgets\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 120   # safety cap on steps per episode (will often terminate earlier)\n",
    "buffer_capacity = 50000\n",
    "step_budget = 1_500_000       # max training steps (across all agents)\n",
    "collision_budget = 4000       # max total collisions during training\n",
    "walltime_budget = 600         # max wall-clock time (seconds) for training\n",
    "\n",
    "# Initialize environment, replay buffer, epsilon scheduler, and metric logger\n",
    "env = GridWorldEnvironment()\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "eps_scheduler = EpsilonScheduler(start=1.0, end=0.1, decay=0.995)\n",
    "logger = MetricLogger()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "# Training loop\n",
    "import time\n",
    "start_time = time.time()\n",
    "total_steps = 0\n",
    "total_collisions = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    # Randomize A and B locations for this episode (ensuring A != B)\n",
    "    while True:\n",
    "        a_row = np.random.randint(env.n)\n",
    "        a_col = np.random.randint(env.m)\n",
    "        b_row = np.random.randint(env.n)\n",
    "        b_col = np.random.randint(env.m)\n",
    "        if (a_row, a_col) != (b_row, b_col):\n",
    "            break\n",
    "    env.food_source_location = (a_row, a_col)\n",
    "    env.nest_location = (b_row, b_col)\n",
    "    env._reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_collisions = 0\n",
    "    episode_round_trips = 0\n",
    "    eps = eps_scheduler.get_epsilon()  # current epsilon for this episode\n",
    "    done_episode = False\n",
    "\n",
    "    # Loop for agent moves within the episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Each iteration allows all agents to move once (round-robin order)\n",
    "        for _ in range(env.num_agents):\n",
    "            agent_id = env.get_next_agent()\n",
    "            state = env.get_state(agent_id)\n",
    "            # ϵ-greedy action selection\n",
    "            if random.random() < eps:\n",
    "                action = random.randrange(action_dim)\n",
    "            else:\n",
    "                # Use policy net to select best action\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = int(torch.argmax(q_values).item())\n",
    "            # Execute action\n",
    "            reward = env.take_action(agent_id, action)\n",
    "            # Check for collisions caused by this action\n",
    "            env.check_collisions()\n",
    "            if env.collision_count > episode_collisions:\n",
    "                # A new collision occurred in this step\n",
    "                new_collisions = env.collision_count - episode_collisions\n",
    "                total_collisions += new_collisions\n",
    "                episode_collisions = env.collision_count\n",
    "                # Apply collision penalties to agents involved\n",
    "                for ag in env.agents:\n",
    "                    if ag.collision_penalty != 0:\n",
    "                        # Add penalty to moving agent's reward (already included in env.take_action via reward if same agent)\n",
    "                        if ag.agent_id != agent_id:\n",
    "                            # If another agent was collided into, include its penalty in total episode reward (it will not act this step)\n",
    "                            reward += ag.collision_penalty\n",
    "                        episode_reward += ag.collision_penalty\n",
    "                        ag.collision_penalty = 0  # reset after applying\n",
    "            # Observe next state\n",
    "            next_state = env.get_state(agent_id)\n",
    "            done = False  # we don't use 'done' for per-agent steps in training\n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            # Increment episodic totals\n",
    "            episode_reward += reward\n",
    "\n",
    "            # DQN training step: sample a batch and update the network\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                # Compute current Q estimates and target Q values\n",
    "                state_batch = torch.tensor(states, dtype=torch.float32)\n",
    "                next_state_batch = torch.tensor(next_states, dtype=torch.float32)\n",
    "                action_batch = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "                reward_batch = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "                done_batch = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "                # Current Q values (policy_net) for taken actions\n",
    "                q_current = policy_net(state_batch).gather(1, action_batch)\n",
    "                # Compute next Q values from target_net\n",
    "                q_next = target_net(next_state_batch).max(dim=1, keepdim=True)[0]\n",
    "                # Compute target Q: r + γ * max Q_next (0 if done)\n",
    "                q_target = reward_batch + gamma * q_next * (1 - done_batch)\n",
    "                # Optimize the policy network\n",
    "                loss = loss_fn(q_current, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_steps += 1\n",
    "            # Check if any agent finished a round trip in this step\n",
    "            if env.round_trip_count > 0:\n",
    "                done_episode = True\n",
    "            # Check global stopping criteria\n",
    "            if total_steps >= step_budget or total_collisions >= collision_budget or (time.time() - start_time) >= walltime_budget:\n",
    "                done_episode = True\n",
    "            if done_episode:\n",
    "                break  # break out of for _ in range(num_agents)\n",
    "        # Sync target network periodically\n",
    "        if total_steps % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        if done_episode or step == max_steps_per_episode - 1:\n",
    "            # Record round trips completed in this episode (if any)\n",
    "            episode_round_trips = env.round_trip_count\n",
    "            break  # break out of step loop\n",
    "\n",
    "    # Log metrics for this episode\n",
    "    logger.log_episode(episode_reward, episode_collisions, episode_round_trips, step+1)\n",
    "    # Print progress every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode:3d}: Reward={episode_reward:.0f}, Collisions={episode_collisions}, RoundTrips={episode_round_trips}, Epsilon={eps:.2f}\")\n",
    "\n",
    "    # Check training termination conditions after each episode\n",
    "    if total_steps >= step_budget or total_collisions >= collision_budget or (time.time() - start_time) >= walltime_budget:\n",
    "        print(\"Stopping training early due to budget limit reached.\")\n",
    "        break\n",
    "\n",
    "# Save trained model weights\n",
    "torch.save(policy_net.state_dict(), \"trained_model.pth\")\n",
    "print(\"Training completed.\")\n",
    "stats = logger.get_stats()\n",
    "print(f\"Average Reward: {stats['mean_reward']:.2f}, Average Collisions: {stats['mean_collisions']:.2f}, Average RoundTrips: {stats['mean_round_trips']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5b962-72a4-458c-8bee-b383e0278f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of Trained Policy\n",
    "\n",
    "# Load the trained model for evaluation\n",
    "trained_model = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "trained_model.load_state_dict(torch.load(\"trained_model.pth\", map_location=\"cpu\"))\n",
    "trained_model.eval()\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"Evaluate the trained model on all possible A-B configurations and specified agent distributions.\"\"\"\n",
    "    GRID_SIZE = 5\n",
    "    distributions = [(1,3), (2,2), (3,1), (4,0)]  # (agents at B, agents at A)\n",
    "    success_count = 0\n",
    "    total_tests = 0\n",
    "    stats = {\n",
    "        'total_tests': 0,\n",
    "        'successful_tests': 0,\n",
    "        'failures': {'collisions': 0, 'timeout': 0, 'incomplete': 0},\n",
    "        'distribution_success': {'1,3': 0, '2,2': 0, '3,1': 0, '4,0': 0},\n",
    "        'avg_steps_successful': 0\n",
    "    }\n",
    "    # Helper: choose greedy action using the trained model\n",
    "    def greedy_action(state):\n",
    "        state_t = torch.tensor(state, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            q_vals = model(state_t)\n",
    "        return int(torch.argmax(q_vals).item())\n",
    "\n",
    "    # Iterate over all possible A, B placements\n",
    "    for ax in range(GRID_SIZE):\n",
    "        for ay in range(GRID_SIZE):\n",
    "            for bx in range(GRID_SIZE):\n",
    "                for by in range(GRID_SIZE):\n",
    "                    if (ax, ay) == (bx, by):\n",
    "                        continue  # skip invalid configs where A == B\n",
    "                    for (b_agents, a_agents) in distributions:\n",
    "                        total_tests += 1\n",
    "                        stats['total_tests'] = total_tests\n",
    "                        # Initialize env with given A and B\n",
    "                        test_env = GridWorldEnvironment(n=GRID_SIZE, m=GRID_SIZE, num_agents=4,\n",
    "                                                        food_source_location=(ax, ay),\n",
    "                                                        nest_location=(bx, by))\n",
    "                        # Manually set agents at B or A according to the distribution\n",
    "                        # First agent (index 0) at B, since we test that agent's ability to do B->A->B\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            if i < b_agents:\n",
    "                                # place this agent at B (nest), without item\n",
    "                                test_env.agents[i].position = test_env.nest_location\n",
    "                                test_env.agents[i].has_item = 0\n",
    "                                test_env.agents[i].direction = False\n",
    "                            else:\n",
    "                                # place this agent at A (food source), with item\n",
    "                                test_env.agents[i].position = test_env.food_source_location\n",
    "                                test_env.agents[i].has_item = 1\n",
    "                                test_env.agents[i].direction = True\n",
    "                        # Reset central clock and counters for safety\n",
    "                        test_env.clock = 0\n",
    "                        test_env.collision_count = 0\n",
    "                        test_env.round_trip_count = 0\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            test_env.agent_states[i] = 0\n",
    "                            test_env.agent_round_trips[i] = False\n",
    "\n",
    "                        # Simulate until first agent (index 0) completes B->A->B or until 25 steps per agent\n",
    "                        agent0_success = False\n",
    "                        collided = False\n",
    "                        max_steps = 25  # max moves per agent\n",
    "                        steps_taken = 0\n",
    "                        for t in range(max_steps * test_env.num_agents):\n",
    "                            aid = test_env.get_next_agent()\n",
    "                            state = test_env.get_state(aid)\n",
    "                            action = greedy_action(state)\n",
    "                            test_env.take_action(aid, action)\n",
    "                            test_env.check_collisions()\n",
    "                            # If any collision occurs, note if agent0 was involved\n",
    "                            if test_env.collision_count > 0:\n",
    "                                for ag in test_env.agents:\n",
    "                                    if ag.collision_penalty != 0:\n",
    "                                        if ag.agent_id == 0:\n",
    "                                            collided = True\n",
    "                                        ag.collision_penalty = 0\n",
    "                            # Check if agent 0 completed B->A->B (round trip)\n",
    "                            # B->A->B is completed when agent0 goes from B (start) -> A (pickup) -> B (drop-off).\n",
    "                            # In our env logic, that corresponds to agent_states[0] reaching 3 (completed) or round_trip_count increment.\n",
    "                            if test_env.agent_round_trips[0]:\n",
    "                                agent0_success = True\n",
    "                                break\n",
    "                            steps_taken += 1\n",
    "                        # Determine success/failure for this test\n",
    "                        if agent0_success and not collided:\n",
    "                            success_count += 1\n",
    "                            stats['successful_tests'] += 1\n",
    "                            stats['distribution_success'][f'{b_agents},{a_agents}'] += 1\n",
    "                            stats['avg_steps_successful'] += steps_taken\n",
    "                        else:\n",
    "                            # Determine failure reason\n",
    "                            if not agent0_success and steps_taken >= max_steps * test_env.num_agents:\n",
    "                                stats['failures']['timeout'] += 1\n",
    "                            elif collided:\n",
    "                                stats['failures']['collisions'] += 1\n",
    "                            else:\n",
    "                                stats['failures']['incomplete'] += 1\n",
    "\n",
    "    # Calculate success rate and average steps\n",
    "    success_rate = success_count / total_tests\n",
    "    if stats['successful_tests'] > 0:\n",
    "        stats['avg_steps_successful'] /= stats['successful_tests']\n",
    "    return success_rate, stats\n",
    "\n",
    "# Run evaluation\n",
    "success_rate, eval_stats = evaluate_model(trained_model)\n",
    "print(f\"Success Rate: {success_rate*100:.2f}%\")\n",
    "print(\"Success rate per distribution:\", {dist: eval_stats['distribution_success'][dist] for dist in eval_stats['distribution_success']})\n",
    "print(\"Failures breakdown:\", eval_stats['failures'])\n",
    "print(f\"Average steps for successful episodes: {eval_stats['avg_steps_successful']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3af05-83d2-40c6-a8ed-5fa605c70d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
