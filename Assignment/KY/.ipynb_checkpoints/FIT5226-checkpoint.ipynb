{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c002af5-0751-4134-bcea-8b2214020d46",
   "metadata": {},
   "source": [
    "# FIT5226 Assignment 1 #\n",
    "\n",
    "## Project Overview ##\n",
    "This project implements a Deep Q-Network (DQN) model in a multi-agent GridWorld shuttle environment. \n",
    "There are 4 agents, one pick-up point A and drop-off point B in a 5X5 grid world. \n",
    "Each agent learns to shuttle items between two locations (A and B) repeatedly while avoiding head-on collisions with other agents moving in the opposite direction.\n",
    "\n",
    "Two early bird options are purchased and used while training the model \n",
    "- State of neighbouring cells checked for agents of opposite type\n",
    "- Central clock\n",
    "\n",
    "The final policy must demonstrate at least 75% success rate in collision-free delivery scenarios and completed a round-trip of within 25 steps to meet the minimum performance benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409b6af-a32d-401f-8c84-8d276e0d31cf",
   "metadata": {},
   "source": [
    "## 1. Setting environment and agents ##\n",
    "\n",
    "Initiating an environment of `n*m` grid world with `num_agent` of agents.\n",
    "\n",
    "Central clock option is being used as a round-robin update `get_next_agent` so that each agent acts in a fixed sequence each timestep. A reward structure of Dropoff (+60) > Pickup (+40) > Step (-1) > Collision (-50) is implemented while training the agent. The agent’s direction (toward B if carrying item, or toward A if not) is updated after each move, which is used by `get_local_mask` to mark opposite-direction neighbors.\n",
    "\n",
    "Collisions are checked after each agent’s move via `check_collisions`. If two agents of opposite direction (`has_item` or not) land on the same cell (except for point A & B), both receive a collision penalty of –50 which is accumulated in their collision_penalty attribute.\n",
    "\n",
    "There are the **15** states that make up the state vector:\n",
    "1. `x`: Current x position of the agent\n",
    "2. `y`: Current y position of the agent\n",
    "3. `food_source_location[0]`: x coordinate of food source (A)\n",
    "4. `food_source_location[1]`: y coordinate of food source (A)\n",
    "5. `nest_location[0]`: x coordinate of nest (B)\n",
    "6. `nest_location[1]`: y coordinate of nest (B)\n",
    "7. `has_item`: Binary flag (0 or 1) indicating if agent is carrying an item\n",
    "8. 8-15 binary flags indicating presence of opposite-direction agents in the 8 neighboring cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40d40f18-18f0-4900-8ec0-79f11ae5e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and Agent Definitions\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define agent class to hold state\n",
    "class QTableAgent:\n",
    "    def __init__(self, agent_id):\n",
    "        self.agent_id = agent_id\n",
    "        self.has_item = 0        # 1 if carrying an item, else 0\n",
    "        self.direction = None    # True if heading A->B, False if heading B->A\n",
    "        self.position = None\n",
    "        self.previous_position = None\n",
    "        self.local_mask = 0      # 8-bit mask for opposite-direction neighbors\n",
    "        self.update_order = None # order index for central clock updates\n",
    "        self.collision_penalty = 0\n",
    "        self.completed_round_trip = False\n",
    "\n",
    "# Grid world environment for multi-agent shuttle\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, n=5, m=5, num_agents=4, food_source_location=None, nest_location=None):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.num_agents = num_agents\n",
    "        # Set A (food source) and B (nest) locations\n",
    "        self.food_source_location = food_source_location if food_source_location is not None else (0, 0)\n",
    "        self.nest_location = nest_location if nest_location is not None else (n-1, m-1)\n",
    "        # Initialize agents list\n",
    "        self.agents = [QTableAgent(i) for i in range(num_agents)]\n",
    "        # Define relative positions for the 8 neighboring directions (N, NE, E, SE, S, SW, W, NW)\n",
    "        self.directions = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                            (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        # Central clock for coordinated updates\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(num_agents))  # default round-robin order\n",
    "        # Collision and round-trip tracking\n",
    "        self.collision_count = 0\n",
    "        self.collision_penalty_value = -50  # penalty applied to each agent in a collision\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * num_agents       # progress state (0->3) for each agent's round trip\n",
    "        self.agent_round_trips = [False] * num_agents  # whether each agent completed a round trip\n",
    "        # Place agents at start positions\n",
    "        self._reset()  # randomize initial agent placements at A or B\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset agents to random start at A or B, and reset environment counters.\"\"\"\n",
    "        # Randomize each agent's starting position: 50% at A (with item), 50% at B (empty)\n",
    "        for agent in self.agents:\n",
    "            if random.random() < 0.5:\n",
    "                agent.position = self.food_source_location\n",
    "                agent.has_item = 1\n",
    "                agent.direction = True   # carrying item, so heading toward B\n",
    "            else:\n",
    "                agent.position = self.nest_location\n",
    "                agent.has_item = 0\n",
    "                agent.direction = False  # not carrying, so heading toward A\n",
    "            agent.previous_position = None\n",
    "            agent.local_mask = 0\n",
    "            agent.collision_penalty = 0\n",
    "            agent.completed_round_trip = False\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(self.num_agents))\n",
    "\n",
    "    \n",
    "    def reset_environment_counters(self):\n",
    "        \"\"\"Reset only environment counters, not agent positions\"\"\"\n",
    "        self.collision_count = 0\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * self.num_agents\n",
    "        self.agent_round_trips = [False] * self.num_agents\n",
    "\n",
    "    def get_next_agent(self):\n",
    "        \"\"\"Return the next agent ID to act, based on the central clock (round-robin).\"\"\"\n",
    "        agent_id = self.update_sequence[self.clock % self.num_agents]\n",
    "        self.clock += 1\n",
    "        return agent_id\n",
    "\n",
    "    def get_local_mask(self, agent_id):\n",
    "        \"\"\"\n",
    "        Compute an 8-bit mask for agent agent_id indicating which of the 8 neighboring cells\n",
    "        contain an agent moving in the opposite direction (potential head-on collision threat).\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        x, y = agent.position\n",
    "        mask = 0\n",
    "        # Check all neighbors\n",
    "        for i, (dx, dy) in enumerate(self.directions):\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.n and 0 <= ny < self.m:\n",
    "                for other in self.agents:\n",
    "                    if other.agent_id != agent_id:\n",
    "                        if other.position == (nx, ny) and other.direction != agent.direction:\n",
    "                            # Neighbor cell occupied by an opposite-direction agent\n",
    "                            mask |= (1 << i)\n",
    "        return mask\n",
    "\n",
    "    def get_state(self, agent_id):\n",
    "        \"\"\"Return the 15-dimensional state vector for the given agent.\"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Update the agent's local mask (for visualization/debugging if needed)\n",
    "        agent.local_mask = self.get_local_mask(agent_id)\n",
    "        # Construct state: position (x,y), A coords, B coords, carry flag, and 8 neighbor flags\n",
    "        x, y = agent.position\n",
    "        state = [\n",
    "            x, y,\n",
    "            self.food_source_location[0], self.food_source_location[1],\n",
    "            self.nest_location[0], self.nest_location[1],\n",
    "            agent.has_item\n",
    "        ]\n",
    "        # Append 8 binary flags indicating opposite-direction agents around\n",
    "        for i in range(8):\n",
    "            flag = (agent.local_mask >> i) & 1\n",
    "            state.append(flag)\n",
    "        return state\n",
    "\n",
    "    def check_collisions(self):\n",
    "        \"\"\"\n",
    "        Detect head-on collisions: if any non-endpoint cell is occupied by at least one agent with an item \n",
    "        and at least one without an item (i.e., agents heading in opposite directions). \n",
    "        Apply collision penalty to those agents.\n",
    "        \"\"\"\n",
    "        positions = {}\n",
    "        # Group agents by their current position\n",
    "        for agent in self.agents:\n",
    "            positions.setdefault(agent.position, []).append(agent)\n",
    "        # Check each position that has more than one agent\n",
    "        for pos, agents in positions.items():\n",
    "            if len(agents) < 2:\n",
    "                continue\n",
    "            # Ignore collisions at A or B (agents clustering at endpoints is allowed)\n",
    "            if pos == self.food_source_location or pos == self.nest_location:\n",
    "                continue\n",
    "            # Check if there's at least one carrying and one not carrying agent\n",
    "            carrying = any(a.has_item for a in agents)\n",
    "            not_carrying = any(not a.has_item for a in agents)\n",
    "            if carrying and not_carrying:\n",
    "                # Head-on collision detected\n",
    "                self.collision_count += 1\n",
    "                for agent in agents:\n",
    "                    agent.collision_penalty += self.collision_penalty_value\n",
    "\n",
    "    def take_action(self, agent_id, action):\n",
    "        \"\"\"\n",
    "        Execute the given action for agent agent_id. \n",
    "        Moves the agent, applies pickup/dropoff, updates round-trip state, and returns the reward.\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Save previous position (for potential collision checks or info)\n",
    "        agent.previous_position = agent.position\n",
    "        x, y = agent.position\n",
    "        # Determine next position based on action\n",
    "        if action == 0:   # Up\n",
    "            nx, ny = max(x - 1, 0), y\n",
    "        elif action == 1: # Down\n",
    "            nx, ny = min(x + 1, self.n - 1), y\n",
    "        elif action == 2: # Left\n",
    "            nx, ny = x, max(y - 1, 0)\n",
    "        elif action == 3: # Right\n",
    "            nx, ny = x, min(y + 1, self.m - 1)\n",
    "        else:\n",
    "            nx, ny = x, y  # no-op for safety (should not happen)\n",
    "        next_pos = (nx, ny)\n",
    "        # Default step penalty\n",
    "        reward = -1\n",
    "        # Check for pickup or drop-off events\n",
    "        if next_pos == self.food_source_location and agent.has_item == 0:\n",
    "            # Arriving at A without an item -> pick up item\n",
    "            agent.has_item = 1\n",
    "            reward += 40   # net +39 for pickup (reward was -1, now -1+40)\n",
    "        elif next_pos == self.nest_location and agent.has_item == 1:\n",
    "            # Arriving at B with an item -> drop off\n",
    "            agent.has_item = 0\n",
    "            reward += 60   # net +59 for successful delivery\n",
    "        # Update agent's position\n",
    "        agent.position = next_pos\n",
    "        # Update agent's travel direction based on carry status\n",
    "        agent.direction = True if agent.has_item else False\n",
    "\n",
    "        # Update agent's round-trip progress state machine:\n",
    "        # States: 0 (not started or reset), 1 (halfway), 2 (reached opposite end), 3 (completed round trip).\n",
    "        # A -> B -> A trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 1  # Picked up at A, heading to B\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 2  # Dropped off at B (halfway done A->B->A)\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.food_source_location and agent.has_item == 0:\n",
    "            # Came back to A without item -> completed A->B->A round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        # B -> A -> B trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 1  # Started at B, heading to A\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 2  # Picked up at A, heading back to B\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            # Returned to B with item delivered -> completed B->A->B round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36506ee-b854-476f-b883-a55e6b4ca0a2",
   "metadata": {},
   "source": [
    "## 2. DQN Agent Setup and Replay Buffer ##\n",
    "\n",
    "During training, the agent's policy is updated via experience replay and periodic target network synchronization. `ReplayBuffer` is set up to store experiences (state, action, reward, next_state, done). The DQN policy network `policy_net` is a feed-forward neural net with two hidden layers of 128 neurons each (ReLU activations) and an output layer of size 4. A target network `target_net` with the same architecture are periodically updated (copied from the policy network) to stabilize training.\n",
    "\n",
    "\n",
    "An epsilon-greedy strategy controls exploration and gradually decays to promote exploitation of learned behaviors via the implementation of `EpsilonScheduler`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2df5fecf-4c58-4b08-abd1-a22444f17cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Replay buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            # Remove oldest experience if at capacity\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states      = np.array([exp[0] for exp in batch], dtype=np.float32)\n",
    "        actions     = np.array([exp[1] for exp in batch], dtype=np.int64)\n",
    "        rewards     = np.array([exp[2] for exp in batch], dtype=np.float32)\n",
    "        next_states = np.array([exp[3] for exp in batch], dtype=np.float32)\n",
    "        dones       = np.array([exp[4] for exp in batch], dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize DQN policy network and target network\n",
    "state_dim = 15   # state features (from environment state vector)\n",
    "action_dim = 4   # four possible actions\n",
    "policy_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "target_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "# Copy weights from policy to target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# DQN hyperparameters\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.95             # discount factor\n",
    "batch_size = 64\n",
    "target_update_freq = 250 # how often to sync target net (in updates)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Epsilon-greedy exploration schedule\n",
    "class EpsilonScheduler:\n",
    "    def __init__(self, start=1.0, end=0.1, decay=0.995):\n",
    "        self.epsilon = start\n",
    "        self.min_epsilon = end\n",
    "        self.decay = decay\n",
    "    def get_epsilon(self):\n",
    "        # Return current epsilon and decay it for next call\n",
    "        eps = self.epsilon\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.decay)\n",
    "        return eps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c9235-17fe-4232-83e2-84108e3d2c89",
   "metadata": {},
   "source": [
    "## 3. Training Loop ##\n",
    "\n",
    "At the start of each episode, we randomize the positions of A and B on the 5×5 grid.\n",
    "\n",
    "One episode terminates when either 1 of the followings is triggered:\n",
    "- At least one agent completes a round trip (A→B→A or B→A→B)\n",
    "- The episode reaches 120 steps (30 steps per agent)\n",
    "- Reach total step budget limit of 1.5 million\n",
    "- Reach total collision budget limit of 4000\n",
    "- Reach total training time limit of 10 minutes\n",
    "\n",
    "\n",
    "For each new episode, a new set of position A & B are placed and `env.reset_environment_counters` is called. A decayed value of epsilon is retrieved via `eps_scheduler`. \n",
    "\n",
    "We loop over the number of agents, and each time call `env.get_next_agent` to get the next agent ID in round-robin order to act. We fetch that agent’s state and choose an action via ϵ-greedy policy. If a random number < ϵ, we take a random action; otherwise, we feed the state to policy_net to get Q-values and pick the argmax action. This means early in training (high ϵ) agents explore randomly, and later on they exploit the learned Q-values more often.\n",
    "\n",
    "We then call `env.take_action` which returns a reward and `env.check_collisions` to update collision penalties. After obtaining the next_state for the agent after the move via `env.get_state`, we push the experience (state, action, reward, next_state, done) into the `replay_buffer`. If the buffer has at least batch_size experiences, we sample a batch and perform a DQN update as follow:\n",
    "\n",
    "\n",
    "- Compute Q-values for the actions taken: use `policy_net` and `.gather` to pick the columns corresponding to each action\n",
    "- Compute TD target: use `target_net` to get the max Q for next states, multiply by gamma and add the reward\n",
    "- Compute loss: call `loss_fn` and backpropagate to update the policy_net\n",
    "\n",
    "At this point, we expect the DQN policy to be learned. The agents should reliably navigate from B to A to pick up, then to B to drop off, while coordinating via the central clock to avoid colliding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "edee3e26-6f8c-4251-8248-00458bdac9e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Ep    1- 100 | Epsilon 0.61 | Reward   -95813 | Coll  543 | RT   99 | AvgLoss 532.5375 | Steps 7,113 | CumColl 543 | Time    9.1s\n",
      "Ep  101- 200 | Epsilon 0.37 | Reward   -48838 | Coll  273 | RT   99 | AvgLoss 431.1715 | Steps 12,681 | CumColl 816 | Time   16.8s\n",
      "Ep  201- 300 | Epsilon 0.22 | Reward   -40793 | Coll  239 | RT  100 | AvgLoss 379.6672 | Steps 18,064 | CumColl 1,055 | Time   24.0s\n",
      "Ep  301- 400 | Epsilon 0.14 | Reward   -27767 | Coll  164 | RT   97 | AvgLoss 332.0041 | Steps 26,091 | CumColl 1,219 | Time   35.9s\n",
      "Ep  401- 500 | Epsilon 0.08 | Reward   -14954 | Coll  106 | RT  100 | AvgLoss 291.6419 | Steps 31,575 | CumColl 1,325 | Time   43.4s\n",
      "Ep  501- 600 | Epsilon 0.05 | Reward      308 | Coll   36 | RT   99 | AvgLoss 271.9303 | Steps 37,677 | CumColl 1,361 | Time   51.3s\n",
      "Ep  601- 700 | Epsilon 0.05 | Reward     2069 | Coll   32 | RT   99 | AvgLoss 238.6016 | Steps 42,088 | CumColl 1,393 | Time   57.0s\n",
      "Ep  701- 800 | Epsilon 0.05 | Reward     7210 | Coll   17 | RT   99 | AvgLoss 233.4736 | Steps 46,258 | CumColl 1,410 | Time   62.5s\n",
      "Ep  801- 900 | Epsilon 0.05 | Reward     7683 | Coll   23 | RT  100 | AvgLoss 219.9826 | Steps 49,865 | CumColl 1,433 | Time   67.3s\n",
      "Ep  901-1000 | Epsilon 0.05 | Reward    10159 | Coll   12 | RT  100 | AvgLoss 204.2186 | Steps 53,206 | CumColl 1,445 | Time   71.5s\n",
      "Ep 1001-1100 | Epsilon 0.05 | Reward     6935 | Coll   32 | RT  100 | AvgLoss 184.1742 | Steps 56,471 | CumColl 1,477 | Time   75.8s\n",
      "Ep 1101-1200 | Epsilon 0.05 | Reward     5751 | Coll   40 | RT  100 | AvgLoss 173.8439 | Steps 59,480 | CumColl 1,517 | Time   79.7s\n",
      "Ep 1201-1300 | Epsilon 0.05 | Reward    11048 | Coll   24 | RT  100 | AvgLoss 156.5204 | Steps 62,622 | CumColl 1,541 | Time   83.8s\n",
      "Ep 1301-1400 | Epsilon 0.05 | Reward     8780 | Coll   38 | RT  100 | AvgLoss 149.7312 | Steps 65,442 | CumColl 1,579 | Time   87.5s\n",
      "Ep 1401-1500 | Epsilon 0.05 | Reward     9376 | Coll   29 | RT  100 | AvgLoss 145.2797 | Steps 67,906 | CumColl 1,608 | Time   90.7s\n",
      "Ep 1501-1600 | Epsilon 0.05 | Reward    11248 | Coll   27 | RT  100 | AvgLoss 136.5613 | Steps 70,348 | CumColl 1,635 | Time   94.1s\n",
      "Ep 1601-1700 | Epsilon 0.05 | Reward    14133 | Coll   19 | RT  100 | AvgLoss 139.3455 | Steps 72,725 | CumColl 1,654 | Time   97.2s\n",
      "Ep 1701-1800 | Epsilon 0.05 | Reward    11292 | Coll   31 | RT  100 | AvgLoss 134.7836 | Steps 75,223 | CumColl 1,685 | Time  100.4s\n",
      "Ep 1801-1900 | Epsilon 0.05 | Reward     9589 | Coll   48 | RT  100 | AvgLoss 140.3678 | Steps 77,644 | CumColl 1,733 | Time  103.7s\n",
      "Ep 1901-2000 | Epsilon 0.05 | Reward    11697 | Coll   40 | RT  100 | AvgLoss 151.4975 | Steps 80,147 | CumColl 1,773 | Time  106.9s\n",
      "Ep 2001-2100 | Epsilon 0.05 | Reward     9247 | Coll   46 | RT  100 | AvgLoss 144.7377 | Steps 82,490 | CumColl 1,819 | Time  110.6s\n",
      "Ep 2101-2200 | Epsilon 0.05 | Reward    11566 | Coll   37 | RT  100 | AvgLoss 144.5122 | Steps 84,834 | CumColl 1,856 | Time  113.9s\n",
      "Ep 2201-2300 | Epsilon 0.05 | Reward     9421 | Coll   49 | RT  100 | AvgLoss 150.8099 | Steps 87,313 | CumColl 1,905 | Time  117.2s\n",
      "Ep 2301-2400 | Epsilon 0.05 | Reward     9639 | Coll   56 | RT  100 | AvgLoss 150.4580 | Steps 89,844 | CumColl 1,961 | Time  120.7s\n",
      "Ep 2401-2500 | Epsilon 0.05 | Reward     8551 | Coll   63 | RT  100 | AvgLoss 167.5806 | Steps 92,273 | CumColl 2,024 | Time  124.1s\n",
      "Ep 2501-2600 | Epsilon 0.05 | Reward     3627 | Coll   67 | RT  100 | AvgLoss 177.1045 | Steps 94,496 | CumColl 2,091 | Time  127.2s\n",
      "Ep 2601-2700 | Epsilon 0.05 | Reward     9403 | Coll   56 | RT  100 | AvgLoss 183.6164 | Steps 97,153 | CumColl 2,147 | Time  130.9s\n",
      "Ep 2701-2800 | Epsilon 0.05 | Reward     7738 | Coll   56 | RT  100 | AvgLoss 188.7502 | Steps 99,855 | CumColl 2,203 | Time  134.4s\n",
      "Ep 2801-2900 | Epsilon 0.05 | Reward    15116 | Coll   33 | RT  100 | AvgLoss 193.5663 | Steps 102,439 | CumColl 2,236 | Time  137.8s\n",
      "Ep 2901-3000 | Epsilon 0.05 | Reward    12085 | Coll   37 | RT  100 | AvgLoss 191.7110 | Steps 104,914 | CumColl 2,273 | Time  141.2s\n",
      "Ep 3001-3100 | Epsilon 0.05 | Reward     7163 | Coll   72 | RT  100 | AvgLoss 199.5949 | Steps 107,131 | CumColl 2,345 | Time  144.2s\n",
      "Ep 3101-3200 | Epsilon 0.05 | Reward    12221 | Coll   48 | RT  100 | AvgLoss 203.3730 | Steps 109,540 | CumColl 2,393 | Time  147.4s\n",
      "Ep 3201-3300 | Epsilon 0.05 | Reward    13551 | Coll   44 | RT  100 | AvgLoss 206.5316 | Steps 112,009 | CumColl 2,437 | Time  150.7s\n",
      "Ep 3301-3400 | Epsilon 0.05 | Reward     6998 | Coll   71 | RT  100 | AvgLoss 206.8965 | Steps 114,521 | CumColl 2,508 | Time  154.1s\n",
      "Ep 3401-3500 | Epsilon 0.05 | Reward     8190 | Coll   65 | RT  100 | AvgLoss 217.0593 | Steps 117,171 | CumColl 2,573 | Time  158.0s\n",
      "Ep 3501-3600 | Epsilon 0.05 | Reward    11339 | Coll   48 | RT  100 | AvgLoss 221.2212 | Steps 119,632 | CumColl 2,621 | Time  161.8s\n",
      "Ep 3601-3700 | Epsilon 0.05 | Reward     9992 | Coll   57 | RT  100 | AvgLoss 220.3326 | Steps 122,110 | CumColl 2,678 | Time  165.6s\n",
      "Ep 3701-3800 | Epsilon 0.05 | Reward    10851 | Coll   62 | RT  100 | AvgLoss 221.6116 | Steps 124,759 | CumColl 2,740 | Time  169.5s\n",
      "Ep 3801-3900 | Epsilon 0.05 | Reward    11920 | Coll   49 | RT  100 | AvgLoss 221.1247 | Steps 126,929 | CumColl 2,789 | Time  172.4s\n",
      "Ep 3901-4000 | Epsilon 0.05 | Reward     8911 | Coll   64 | RT  100 | AvgLoss 220.2608 | Steps 129,618 | CumColl 2,853 | Time  176.0s\n",
      "Ep 4001-4100 | Epsilon 0.05 | Reward     6763 | Coll   70 | RT  100 | AvgLoss 228.0154 | Steps 131,905 | CumColl 2,923 | Time  179.0s\n",
      "Ep 4101-4200 | Epsilon 0.05 | Reward    11570 | Coll   45 | RT  100 | AvgLoss 221.8570 | Steps 134,185 | CumColl 2,968 | Time  182.0s\n",
      "Ep 4201-4300 | Epsilon 0.05 | Reward     6373 | Coll   81 | RT  100 | AvgLoss 221.5169 | Steps 136,502 | CumColl 3,049 | Time  185.1s\n",
      "Ep 4301-4400 | Epsilon 0.05 | Reward    12038 | Coll   54 | RT  100 | AvgLoss 231.5585 | Steps 138,824 | CumColl 3,103 | Time  188.1s\n",
      "Ep 4401-4500 | Epsilon 0.05 | Reward     9364 | Coll   61 | RT  100 | AvgLoss 226.4007 | Steps 141,270 | CumColl 3,164 | Time  191.3s\n",
      "Ep 4501-4600 | Epsilon 0.05 | Reward     8977 | Coll   68 | RT  100 | AvgLoss 225.3053 | Steps 143,923 | CumColl 3,232 | Time  194.8s\n",
      "Ep 4601-4700 | Epsilon 0.05 | Reward     9268 | Coll   57 | RT  100 | AvgLoss 219.0960 | Steps 146,015 | CumColl 3,289 | Time  198.0s\n",
      "Ep 4701-4800 | Epsilon 0.05 | Reward     8508 | Coll   68 | RT  100 | AvgLoss 228.2347 | Steps 148,327 | CumColl 3,357 | Time  201.2s\n",
      "Ep 4801-4900 | Epsilon 0.05 | Reward    12926 | Coll   50 | RT  100 | AvgLoss 223.6867 | Steps 150,781 | CumColl 3,407 | Time  204.7s\n",
      "Ep 4901-5000 | Epsilon 0.05 | Reward    12329 | Coll   52 | RT  100 | AvgLoss 230.5071 | Steps 152,842 | CumColl 3,459 | Time  207.5s\n",
      "Ep 5001-5100 | Epsilon 0.05 | Reward    13476 | Coll   50 | RT  100 | AvgLoss 226.4015 | Steps 155,226 | CumColl 3,509 | Time  211.0s\n",
      "Ep 5101-5200 | Epsilon 0.05 | Reward    14600 | Coll   43 | RT  100 | AvgLoss 221.7639 | Steps 157,476 | CumColl 3,552 | Time  214.6s\n",
      "Ep 5201-5300 | Epsilon 0.05 | Reward    10849 | Coll   59 | RT  100 | AvgLoss 224.5780 | Steps 159,817 | CumColl 3,611 | Time  218.0s\n",
      "Ep 5301-5400 | Epsilon 0.05 | Reward    12367 | Coll   45 | RT  100 | AvgLoss 231.7636 | Steps 162,020 | CumColl 3,656 | Time  221.1s\n",
      "Ep 5401-5500 | Epsilon 0.05 | Reward    12429 | Coll   53 | RT  100 | AvgLoss 227.3894 | Steps 164,581 | CumColl 3,709 | Time  224.6s\n",
      "Ep 5501-5600 | Epsilon 0.05 | Reward     9412 | Coll   62 | RT  100 | AvgLoss 224.0149 | Steps 166,689 | CumColl 3,771 | Time  227.4s\n",
      "Ep 5601-5700 | Epsilon 0.05 | Reward    10921 | Coll   60 | RT  100 | AvgLoss 220.1794 | Steps 168,838 | CumColl 3,831 | Time  230.4s\n",
      "Ep 5701-5800 | Epsilon 0.05 | Reward    11864 | Coll   48 | RT  100 | AvgLoss 218.5916 | Steps 171,174 | CumColl 3,879 | Time  233.5s\n",
      "Ep 5801-5900 | Epsilon 0.05 | Reward    14945 | Coll   42 | RT  100 | AvgLoss 218.3819 | Steps 173,579 | CumColl 3,921 | Time  236.7s\n",
      "Ep 5901-6000 | Epsilon 0.05 | Reward    13626 | Coll   50 | RT  100 | AvgLoss 217.0125 | Steps 175,933 | CumColl 3,971 | Time  239.8s\n",
      "Stopping training early due to collision budget limit reached.\n",
      "Training completed.\n",
      "Average Reward: 53.35, Average Collisions: 0.66, Average RoundTrips: 1.00\n",
      "Finished plotting, image saved !\n"
     ]
    }
   ],
   "source": [
    "# Logging utility to record training metrics\n",
    "class MetricLogger:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.collisions = []\n",
    "        self.round_trips = []\n",
    "        self.lengths = []\n",
    "        self.episodes = []\n",
    "    def log_episode(self, total_reward, collisions, round_trips, length):\n",
    "        self.rewards.append(total_reward)\n",
    "        self.collisions.append(collisions)\n",
    "        self.round_trips.append(round_trips)\n",
    "        self.lengths.append(length)\n",
    "        self.episodes.append(len(self.episodes) + 1)\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"mean_reward\": np.mean(self.rewards) if self.rewards else 0.0,\n",
    "            \"mean_collisions\": np.mean(self.collisions) if self.collisions else 0.0,\n",
    "            \"mean_round_trips\": np.mean(self.round_trips) if self.round_trips else 0.0,\n",
    "            \"mean_length\": np.mean(self.lengths) if self.lengths else 0.0\n",
    "        }\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training metrics for every episode\"\"\"\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            \n",
    "        # Plot rewards\n",
    "        axs[0, 0].plot(self.episodes, self.rewards, '-', alpha=0.7)\n",
    "        axs[0, 0].set_title('Total Reward per Episode')\n",
    "        axs[0, 0].set_xlabel('Episode')\n",
    "        axs[0, 0].set_ylabel('Reward')\n",
    "        axs[0, 0].grid(True)\n",
    "        \n",
    "        # Plot collisions\n",
    "        axs[0, 1].plot(self.episodes, self.collisions, '-', alpha=0.7)\n",
    "        axs[0, 1].set_title('Collisions per Episode')\n",
    "        axs[0, 1].set_xlabel('Episode')\n",
    "        axs[0, 1].set_ylabel('Number of Collisions')\n",
    "        axs[0, 1].grid(True)\n",
    "        \n",
    "        # Plot round trips\n",
    "        axs[1, 0].plot(self.episodes, self.round_trips, '-', alpha=0.7)\n",
    "        axs[1, 0].set_title('Round Trips per Episode')\n",
    "        axs[1, 0].set_xlabel('Episode')\n",
    "        axs[1, 0].set_ylabel('Number of Round Trips')\n",
    "        axs[1, 0].grid(True)\n",
    "        \n",
    "        # Plot episode lengths\n",
    "        axs[1, 1].plot(self.episodes, self.lengths, '-', alpha=0.7)\n",
    "        axs[1, 1].set_title('Episode Length')\n",
    "        axs[1, 1].set_xlabel('Episode')\n",
    "        axs[1, 1].set_ylabel('Steps')\n",
    "        axs[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "# Training parameters and budgets\n",
    "num_episodes = 8000\n",
    "max_steps_per_episode = 120   # safety cap on steps per episode (will often terminate earlier)\n",
    "buffer_capacity = 50000\n",
    "step_budget = 1_500_000       # max training steps (across all agents)\n",
    "collision_budget = 4000       # max total collisions during training\n",
    "walltime_budget = 600         # max wall-clock time (seconds) for training\n",
    "\n",
    "# Initialize environment, replay buffer, epsilon scheduler, and metric logger\n",
    "env = GridWorldEnvironment()\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "eps_scheduler = EpsilonScheduler(start=1.0, end=0.05, decay=0.995)\n",
    "logger = MetricLogger()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "# Training loop\n",
    "import time\n",
    "start_time = time.time()\n",
    "total_steps = 0\n",
    "total_collisions = 0\n",
    "\n",
    "# ------------ 100‑episode window accumulators ------------\n",
    "window_reward_sum     = 0.0\n",
    "window_coll_sum       = 0\n",
    "window_roundtrip_sum  = 0\n",
    "window_loss_sum       = 0.0\n",
    "window_loss_count     = 0\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    while True:\n",
    "        a_row = np.random.randint(env.n)\n",
    "        a_col = np.random.randint(env.m)\n",
    "        b_row = np.random.randint(env.n)\n",
    "        b_col = np.random.randint(env.m)\n",
    "        if (a_row, a_col) != (b_row, b_col):\n",
    "            break\n",
    "    env.food_source_location = (a_row, a_col)\n",
    "    env.nest_location = (b_row, b_col)\n",
    "    env.reset_environment_counters()\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_collisions = 0\n",
    "    episode_round_trips = 0\n",
    "    eps = eps_scheduler.get_epsilon()\n",
    "    done_episode = False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        for _ in range(env.num_agents):\n",
    "            agent_id = env.get_next_agent()\n",
    "            state = env.get_state(agent_id)\n",
    "\n",
    "            if random.random() < eps:\n",
    "                action = random.randrange(action_dim)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = int(torch.argmax(q_values).item())\n",
    "\n",
    "            reward = env.take_action(agent_id, action)\n",
    "            env.check_collisions()\n",
    "\n",
    "            if env.collision_count > episode_collisions:\n",
    "                new_collisions = env.collision_count - episode_collisions\n",
    "                total_collisions += new_collisions\n",
    "                episode_collisions = env.collision_count\n",
    "                for ag in env.agents:\n",
    "                    if ag.collision_penalty != 0:\n",
    "                        if ag.agent_id != agent_id:\n",
    "                            reward += ag.collision_penalty\n",
    "                        episode_reward += ag.collision_penalty\n",
    "                        ag.collision_penalty = 0\n",
    "\n",
    "            next_state = env.get_state(agent_id)\n",
    "            done = False\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                state_batch = torch.tensor(states, dtype=torch.float32)\n",
    "                next_state_batch = torch.tensor(next_states, dtype=torch.float32)\n",
    "                action_batch = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "                reward_batch = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "                done_batch = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                q_current = policy_net(state_batch).gather(1, action_batch)\n",
    "                q_next = target_net(next_state_batch).max(dim=1, keepdim=True)[0]\n",
    "                q_target = reward_batch + gamma * q_next * (1 - done_batch)\n",
    "\n",
    "                loss = loss_fn(q_current, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track loss in 100-episode window\n",
    "                window_loss_sum += loss.item()\n",
    "                window_loss_count += 1\n",
    "\n",
    "            total_steps += 1\n",
    "            if env.round_trip_count > 0:\n",
    "                done_episode = True\n",
    "            if total_steps >= step_budget or total_collisions >= collision_budget or (time.time() - start_time) >= walltime_budget:\n",
    "                done_episode = True\n",
    "            if done_episode:\n",
    "                break\n",
    "\n",
    "        if total_steps % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done_episode or step == max_steps_per_episode - 1:\n",
    "            episode_round_trips = env.round_trip_count\n",
    "            break\n",
    "\n",
    "    logger.log_episode(episode_reward, episode_collisions, episode_round_trips, step+1)\n",
    "\n",
    "    # --- Accumulate episode stats for current 100-episode window ---\n",
    "    window_reward_sum    += episode_reward\n",
    "    window_coll_sum      += episode_collisions\n",
    "    window_roundtrip_sum += episode_round_trips\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    # --- Print block every 100 episodes ---\n",
    "    if episode % 100 == 0:\n",
    "        avg_loss = (window_loss_sum / window_loss_count) if window_loss_count else float('nan')\n",
    "        print(f\"Ep {episode-99:4d}-{episode:4d} | \"\n",
    "              f\"Epsilon {eps:.2f} | \"\n",
    "              f\"Reward {window_reward_sum:8.0f} | \"\n",
    "              f\"Coll {window_coll_sum:4d} | \"\n",
    "              f\"RT {window_roundtrip_sum:4d} | \"\n",
    "              f\"AvgLoss {avg_loss:7.4f} | \"\n",
    "              f\"Steps {total_steps:,} | CumColl {total_collisions:,} | \"\n",
    "              f\"Time {time.time()-start_time:6.1f}s\") \n",
    "        # Reset accumulators\n",
    "        window_reward_sum     = 0.0 #\n",
    "        window_coll_sum       = 0\n",
    "        window_roundtrip_sum  = 0\n",
    "        window_loss_sum       = 0.0\n",
    "        window_loss_count     = 0\n",
    "\n",
    "    # Check stop conditions\n",
    "    if total_steps >= step_budget:\n",
    "        print(\"Stopping training early due to step budget limit reached.\")\n",
    "        break\n",
    "    elif total_collisions >= collision_budget:\n",
    "        print (\"Stopping training early due to collision budget limit reached.\")\n",
    "        break\n",
    "    elif (time.time() - start_time) >= walltime_budget:\n",
    "        print (\"Stopping training early due to time budget limit reached.\")\n",
    "        break\n",
    "\n",
    "# Save trained model\n",
    "torch.save(policy_net.state_dict(), \"trained_model.pth\")\n",
    "print(\"Training completed.\")\n",
    "stats = logger.get_stats()\n",
    "print(f\"Average Reward: {stats['mean_reward']:.2f}, \"\n",
    "      f\"Average Collisions: {stats['mean_collisions']:.2f}, \"\n",
    "      f\"Average RoundTrips: {stats['mean_round_trips']:.2f}\")\n",
    "\n",
    "# add visualization\n",
    "logger.plot_metrics()\n",
    "print(f\"Finished plotting, image saved !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1777d9de-f71b-465c-a2d5-1a349470a66e",
   "metadata": {},
   "source": [
    "## Evaluation Metrics ##\n",
    "\n",
    "The following metrics were recorded during training and used to assess the performance of the learned policy:\n",
    "1. Total Reward per Episode: Sum of rewards collected by all agents.\n",
    "2. Collisions per Episode: Number of head-on collisions detected.\n",
    "3. Round Trips Completed: Number of full A→B→A or B→A→B trips completed by any agent.\n",
    "4. Episode Length: Number of steps taken before termination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c68c3-1790-4cbf-9876-f8794c70e37a",
   "metadata": {},
   "source": [
    "## Key Learning Outcomes: ##\n",
    "\n",
    "Looking at the 4 graphs, we can see convergence happen over time\n",
    "\n",
    "1. Total reward per episode increases over time: indicating an increase in efficiency, agents are taking fewer steps (-1) to complete round trip, successfully picking up (+40) and delivering items (+60)  as well as avoiding collisions (-50).\n",
    "3. Collision per episode decreases over time: agents learn to avoid each other\n",
    "4. Episode length increases over time: agents learn to complete round-trips with shorter path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a1eb8-9ca4-47d3-b6bf-f1762d320b87",
   "metadata": {},
   "source": [
    "## 4. Evaluation of Trained Policy ##\n",
    "\n",
    "We load the trained model weights into a fresh network `trained_model` identical in structure to our DQN. During the testing, we are only looking at agent at point B. The evaluate_model function then systematically carry out testing in 2400 scenarios with 600 (25×24) different position of A & B and for each configurations, tests four distributions of agents:\n",
    "- 1 at B, 3 at A\n",
    "- 2 at B, 2 at A\n",
    "- 3 at B, 1 at A\n",
    "- 4 at B, 0 at A\n",
    "\n",
    "For each scenario, we initialize a new `GridWorldEnvironment` with the specified A and B. We manually set the agent positions according to the distribution being tested. We simulate the environment with the agents following the learned policy (no exploration). We allow up to 25 moves per agent (100 moves total for 4 agents). Agents act in round-robin order via `get_next_agent` just like in training. For each agent’s turn, we get the state and choose the action using the trained model.\n",
    "\n",
    "A test case is considered as success if all of the criteria are met:\n",
    "- 1 agent at point B complete B→A→B round trip within 25 steps\n",
    "- No head-on collisions\n",
    "\n",
    "Finally, we compute the overall `success rate` = successes / total_tests, and report the success count for each distribution type, as well as average steps for successful runs and a breakdown of failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "15c5b962-72a4-458c-8bee-b383e0278f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 97.12%\n",
      "Success rate per distribution: {'1,3': 580, '2,2': 580, '3,1': 580, '4,0': 591}\n",
      "Failures breakdown: {'collisions': 38, 'timeout': 31, 'incomplete': 0}\n",
      "Average steps for successful episodes: 23.52\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of Trained Policy\n",
    "\n",
    "# Load the trained model for evaluation\n",
    "trained_model = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "trained_model.load_state_dict(torch.load(\"trained_model.pth\", map_location=\"cpu\"))\n",
    "trained_model.eval()\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"Evaluate the trained model on all possible A-B configurations and specified agent distributions.\"\"\"\n",
    "    GRID_SIZE = 5\n",
    "    distributions = [(1,3), (2,2), (3,1), (4,0)]  # (agents at B, agents at A)\n",
    "    success_count = 0\n",
    "    total_tests = 0\n",
    "    stats = {\n",
    "        'total_tests': 0,\n",
    "        'successful_tests': 0,\n",
    "        'failures': {'collisions': 0, 'timeout': 0, 'incomplete': 0},\n",
    "        'distribution_success': {'1,3': 0, '2,2': 0, '3,1': 0, '4,0': 0},\n",
    "        'avg_steps_successful': 0\n",
    "    }\n",
    "    # Helper: choose greedy action using the trained model\n",
    "    def greedy_action(state):\n",
    "        state_t = torch.tensor(state, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            q_vals = model(state_t)\n",
    "        return int(torch.argmax(q_vals).item())\n",
    "\n",
    "    # Iterate over all possible A, B placements\n",
    "    for ax in range(GRID_SIZE):\n",
    "        for ay in range(GRID_SIZE):\n",
    "            for bx in range(GRID_SIZE):\n",
    "                for by in range(GRID_SIZE):\n",
    "                    if (ax, ay) == (bx, by):\n",
    "                        continue  # skip invalid configs where A == B\n",
    "                    for (b_agents, a_agents) in distributions:\n",
    "                        total_tests += 1\n",
    "                        stats['total_tests'] = total_tests\n",
    "                        # Initialize env with given A and B\n",
    "                        test_env = GridWorldEnvironment(n=GRID_SIZE, m=GRID_SIZE, num_agents=4,\n",
    "                                                        food_source_location=(ax, ay),\n",
    "                                                        nest_location=(bx, by))\n",
    "                        # Manually set agents at B or A according to the distribution\n",
    "                        # First agent (index 0) at B, since we test that agent's ability to do B->A->B\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            if i < b_agents:\n",
    "                                # place this agent at B (nest), without item\n",
    "                                test_env.agents[i].position = test_env.nest_location\n",
    "                                test_env.agents[i].has_item = 0\n",
    "                                test_env.agents[i].direction = False\n",
    "                            else:\n",
    "                                # place this agent at A (food source), with item\n",
    "                                test_env.agents[i].position = test_env.food_source_location\n",
    "                                test_env.agents[i].has_item = 1\n",
    "                                test_env.agents[i].direction = True\n",
    "                        # Reset central clock and counters for safety\n",
    "                        test_env.clock = 0\n",
    "                        test_env.collision_count = 0\n",
    "                        test_env.round_trip_count = 0\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            test_env.agent_states[i] = 0\n",
    "                            test_env.agent_round_trips[i] = False\n",
    "\n",
    "                        # Simulate until first agent (index 0) completes B->A->B or until 25 steps per agent\n",
    "                        agent0_success = False\n",
    "                        collided = False\n",
    "                        max_steps = 25  # max moves per agent\n",
    "                        steps_taken = 0\n",
    "                        for t in range(max_steps * test_env.num_agents):\n",
    "                            aid = test_env.get_next_agent()\n",
    "                            state = test_env.get_state(aid)\n",
    "                            action = greedy_action(state)\n",
    "                            test_env.take_action(aid, action)\n",
    "                            test_env.check_collisions()\n",
    "                            # If any collision occurs, note if agent0 was involved\n",
    "                            if test_env.collision_count > 0:\n",
    "                                for ag in test_env.agents:\n",
    "                                    if ag.collision_penalty != 0:\n",
    "                                        if ag.agent_id == 0:\n",
    "                                            collided = True\n",
    "                                        ag.collision_penalty = 0\n",
    "                            # Check if agent 0 completed B->A->B (round trip)\n",
    "                            # B->A->B is completed when agent0 goes from B (start) -> A (pickup) -> B (drop-off).\n",
    "                            # In our env logic, that corresponds to agent_states[0] reaching 3 (completed) or round_trip_count increment.\n",
    "                            if test_env.agent_round_trips[0]:\n",
    "                                agent0_success = True\n",
    "                                break\n",
    "                            steps_taken += 1\n",
    "                        # Determine success/failure for this test\n",
    "                        if agent0_success and not collided:\n",
    "                            success_count += 1\n",
    "                            stats['successful_tests'] += 1\n",
    "                            stats['distribution_success'][f'{b_agents},{a_agents}'] += 1\n",
    "                            stats['avg_steps_successful'] += steps_taken\n",
    "                        else:\n",
    "                            # Determine failure reason\n",
    "                            if not agent0_success and steps_taken >= max_steps * test_env.num_agents:\n",
    "                                stats['failures']['timeout'] += 1\n",
    "                            elif collided:\n",
    "                                stats['failures']['collisions'] += 1\n",
    "                            else:\n",
    "                                stats['failures']['incomplete'] += 1\n",
    "\n",
    "    # Calculate success rate and average steps\n",
    "    success_rate = success_count / total_tests\n",
    "    if stats['successful_tests'] > 0:\n",
    "        stats['avg_steps_successful'] /= stats['successful_tests']\n",
    "    return success_rate, stats\n",
    "\n",
    "# Run evaluation\n",
    "success_rate, eval_stats = evaluate_model(trained_model)\n",
    "print(f\"Success Rate: {success_rate*100:.2f}%\")\n",
    "print(\"Success rate per distribution:\", {dist: eval_stats['distribution_success'][dist] for dist in eval_stats['distribution_success']})\n",
    "print(\"Failures breakdown:\", eval_stats['failures'])\n",
    "print(f\"Average steps for successful episodes: {eval_stats['avg_steps_successful']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b9f59-c292-424d-a444-3dd289d3c332",
   "metadata": {},
   "source": [
    "## Conclusion ##\n",
    "\n",
    "All in all, we can see that this DQN model generalises well to unseen configurations of A & B locations and agent placements.\n",
    "\n",
    "#### 1. High success rate ####\n",
    "Out of all 2400 possible test configurations, the agents succeeded in 97.12% of cases, indicating the model learned very well. Only 69 failures out of 2,331 total tests (38 collisions + 31 timeouts).\n",
    "\n",
    "#### 2. Consistent Performance Across Distributions ####\n",
    "- 1,3: 580 successes\n",
    "- 2,2: 580 successes\n",
    "- 3,1: 580 successes\n",
    "- 4,0: 591 successes\n",
    "The model performs consistently well regardless of the initial distribution of agents\n",
    "The slightly higher success rate for 4,0 distribution due to the non-existance of head-on collision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7b17f-cfa7-420b-90ab-2d230be82407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
