{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40d40f18-18f0-4900-8ec0-79f11ae5e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and Agent Definitions\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define agent class to hold state\n",
    "class QTableAgent:\n",
    "    def __init__(self, agent_id):\n",
    "        self.agent_id = agent_id\n",
    "        self.has_item = 0        # 1 if carrying an item, else 0\n",
    "        self.direction = None    # True if heading A->B, False if heading B->A\n",
    "        self.position = None\n",
    "        self.previous_position = None\n",
    "        self.local_mask = 0      # 8-bit mask for opposite-direction neighbors\n",
    "        self.update_order = None # order index for central clock updates\n",
    "        self.collision_penalty = 0\n",
    "        self.completed_round_trip = False\n",
    "\n",
    "# Grid world environment for multi-agent shuttle\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, n=5, m=5, num_agents=4, food_source_location=None, nest_location=None):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.num_agents = num_agents\n",
    "        # Set A (food source) and B (nest) locations\n",
    "        self.food_source_location = food_source_location if food_source_location is not None else (0, 0)\n",
    "        self.nest_location = nest_location if nest_location is not None else (n-1, m-1)\n",
    "        # Initialize agents list\n",
    "        self.agents = [QTableAgent(i) for i in range(num_agents)]\n",
    "        # Define relative positions for the 8 neighboring directions (N, NE, E, SE, S, SW, W, NW)\n",
    "        self.directions = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                            (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        # Central clock for coordinated updates\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(num_agents))  # default round-robin order\n",
    "        # Collision and round-trip tracking\n",
    "        self.collision_count = 0\n",
    "        self.collision_penalty_value = -50  # penalty applied to each agent in a collision\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * num_agents       # progress state (0->3) for each agent's round trip\n",
    "        self.agent_round_trips = [False] * num_agents  # whether each agent completed a round trip\n",
    "        # Place agents at start positions\n",
    "        self._reset()  # randomize initial agent placements at A or B\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset agents to random start at A or B, and reset environment counters.\"\"\"\n",
    "        # Randomize each agent's starting position: 50% at A (with item), 50% at B (empty)\n",
    "        for agent in self.agents:\n",
    "            if random.random() < 0.5:\n",
    "                agent.position = self.food_source_location\n",
    "                agent.has_item = 1\n",
    "                agent.direction = True   # carrying item, so heading toward B\n",
    "            else:\n",
    "                agent.position = self.nest_location\n",
    "                agent.has_item = 0\n",
    "                agent.direction = False  # not carrying, so heading toward A\n",
    "            agent.previous_position = None\n",
    "            agent.local_mask = 0\n",
    "            agent.collision_penalty = 0\n",
    "            agent.completed_round_trip = False\n",
    "        self.clock = 0\n",
    "        self.update_sequence = list(range(self.num_agents))\n",
    "\n",
    "    \n",
    "    def reset_environment_counters(self):\n",
    "        \"\"\"Reset only environment counters, not agent positions\"\"\"\n",
    "        self.collision_count = 0\n",
    "        self.round_trip_count = 0\n",
    "        self.agent_states = [0] * self.num_agents\n",
    "        self.agent_round_trips = [False] * self.num_agents\n",
    "\n",
    "    def get_next_agent(self):\n",
    "        \"\"\"Return the next agent ID to act, based on the central clock (round-robin).\"\"\"\n",
    "        agent_id = self.update_sequence[self.clock % self.num_agents]\n",
    "        self.clock += 1\n",
    "        return agent_id\n",
    "\n",
    "    def get_local_mask(self, agent_id):\n",
    "        \"\"\"\n",
    "        Compute an 8-bit mask for agent agent_id indicating which of the 8 neighboring cells\n",
    "        contain an agent moving in the opposite direction (potential head-on collision threat).\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        x, y = agent.position\n",
    "        mask = 0\n",
    "        # Check all neighbors\n",
    "        for i, (dx, dy) in enumerate(self.directions):\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.n and 0 <= ny < self.m:\n",
    "                for other in self.agents:\n",
    "                    if other.agent_id != agent_id:\n",
    "                        if other.position == (nx, ny) and other.direction != agent.direction:\n",
    "                            # Neighbor cell occupied by an opposite-direction agent\n",
    "                            mask |= (1 << i)\n",
    "        return mask\n",
    "\n",
    "    def get_state(self, agent_id):\n",
    "        \"\"\"Return the 15-dimensional state vector for the given agent.\"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Update the agent's local mask (for visualization/debugging if needed)\n",
    "        agent.local_mask = self.get_local_mask(agent_id)\n",
    "        # Construct state: position (x,y), A coords, B coords, carry flag, and 8 neighbor flags\n",
    "        x, y = agent.position\n",
    "        state = [\n",
    "            x, y,\n",
    "            self.food_source_location[0], self.food_source_location[1],\n",
    "            self.nest_location[0], self.nest_location[1],\n",
    "            agent.has_item\n",
    "        ]\n",
    "        # Append 8 binary flags indicating opposite-direction agents around\n",
    "        for i in range(8):\n",
    "            flag = (agent.local_mask >> i) & 1\n",
    "            state.append(flag)\n",
    "        return state\n",
    "\n",
    "    def check_collisions(self):\n",
    "        \"\"\"\n",
    "        Detect head-on collisions: if any non-endpoint cell is occupied by at least one agent with an item \n",
    "        and at least one without an item (i.e., agents heading in opposite directions). \n",
    "        Apply collision penalty to those agents.\n",
    "        \"\"\"\n",
    "        positions = {}\n",
    "        # Group agents by their current position\n",
    "        for agent in self.agents:\n",
    "            positions.setdefault(agent.position, []).append(agent)\n",
    "        # Check each position that has more than one agent\n",
    "        for pos, agents in positions.items():\n",
    "            if len(agents) < 2:\n",
    "                continue\n",
    "            # Ignore collisions at A or B (agents clustering at endpoints is allowed)\n",
    "            if pos == self.food_source_location or pos == self.nest_location:\n",
    "                continue\n",
    "            # Check if there's at least one carrying and one not carrying agent\n",
    "            carrying = any(a.has_item for a in agents)\n",
    "            not_carrying = any(not a.has_item for a in agents)\n",
    "            if carrying and not_carrying:\n",
    "                # Head-on collision detected\n",
    "                self.collision_count += 1\n",
    "                for agent in agents:\n",
    "                    agent.collision_penalty += self.collision_penalty_value\n",
    "\n",
    "    def take_action(self, agent_id, action):\n",
    "        \"\"\"\n",
    "        Execute the given action for agent agent_id. \n",
    "        Moves the agent, applies pickup/dropoff, updates round-trip state, and returns the reward.\n",
    "        \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        # Save previous position (for potential collision checks or info)\n",
    "        agent.previous_position = agent.position\n",
    "        x, y = agent.position\n",
    "        # Determine next position based on action\n",
    "        if action == 0:   # Up\n",
    "            nx, ny = max(x - 1, 0), y\n",
    "        elif action == 1: # Down\n",
    "            nx, ny = min(x + 1, self.n - 1), y\n",
    "        elif action == 2: # Left\n",
    "            nx, ny = x, max(y - 1, 0)\n",
    "        elif action == 3: # Right\n",
    "            nx, ny = x, min(y + 1, self.m - 1)\n",
    "        else:\n",
    "            nx, ny = x, y  # no-op for safety (should not happen)\n",
    "        next_pos = (nx, ny)\n",
    "        # Default step penalty\n",
    "        reward = -1\n",
    "        # Check for pickup or drop-off events\n",
    "        if next_pos == self.food_source_location and agent.has_item == 0:\n",
    "            # Arriving at A without an item -> pick up item\n",
    "            agent.has_item = 1\n",
    "            reward += 40   # net +39 for pickup (reward was -1, now -1+40)\n",
    "        elif next_pos == self.nest_location and agent.has_item == 1:\n",
    "            # Arriving at B with an item -> drop off\n",
    "            agent.has_item = 0\n",
    "            reward += 60   # net +59 for successful delivery\n",
    "        # Update agent's position\n",
    "        agent.position = next_pos\n",
    "        # Update agent's travel direction based on carry status\n",
    "        agent.direction = True if agent.has_item else False\n",
    "\n",
    "        # Update agent's round-trip progress state machine:\n",
    "        # States: 0 (not started or reset), 1 (halfway), 2 (reached opposite end), 3 (completed round trip).\n",
    "        # A -> B -> A trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 1  # Picked up at A, heading to B\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 2  # Dropped off at B (halfway done A->B->A)\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.food_source_location and agent.has_item == 0:\n",
    "            # Came back to A without item -> completed A->B->A round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        # B -> A -> B trip tracking\n",
    "        if self.agent_states[agent_id] == 0 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            self.agent_states[agent_id] = 1  # Started at B, heading to A\n",
    "        elif self.agent_states[agent_id] == 1 and agent.position == self.food_source_location and agent.has_item:\n",
    "            self.agent_states[agent_id] = 2  # Picked up at A, heading back to B\n",
    "        elif self.agent_states[agent_id] == 2 and agent.position == self.nest_location and agent.has_item == 0:\n",
    "            # Returned to B with item delivered -> completed B->A->B round trip\n",
    "            self.agent_states[agent_id] = 3\n",
    "            self.agent_round_trips[agent_id] = True\n",
    "            self.round_trip_count += 1\n",
    "            agent.completed_round_trip = True\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2df5fecf-4c58-4b08-abd1-a22444f17cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent Setup and Replay Buffer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Replay buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            # Remove oldest experience if at capacity\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states      = np.array([exp[0] for exp in batch], dtype=np.float32)\n",
    "        actions     = np.array([exp[1] for exp in batch], dtype=np.int64)\n",
    "        rewards     = np.array([exp[2] for exp in batch], dtype=np.float32)\n",
    "        next_states = np.array([exp[3] for exp in batch], dtype=np.float32)\n",
    "        dones       = np.array([exp[4] for exp in batch], dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize DQN policy network and target network\n",
    "state_dim = 15   # state features (from environment state vector)\n",
    "action_dim = 4   # four possible actions\n",
    "policy_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "target_net = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "# Copy weights from policy to target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# DQN hyperparameters\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.95             # discount factor\n",
    "batch_size = 64\n",
    "target_update_freq = 250 # how often to sync target net (in updates)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Epsilon-greedy exploration schedule\n",
    "class EpsilonScheduler:\n",
    "    def __init__(self, start=1.0, end=0.1, decay=0.995):\n",
    "        self.epsilon = start\n",
    "        self.min_epsilon = end\n",
    "        self.decay = decay\n",
    "    def get_epsilon(self):\n",
    "        # Return current epsilon and decay it for next call\n",
    "        eps = self.epsilon\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.decay)\n",
    "        return eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee3e26-6f8c-4251-8248-00458bdac9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episodes    1- 100 | Epsilon 0.61 | TotalReward   -61946 | TotColl  379 | TotRT  100 | AvgLoss 324.4822 | Steps 4,736 | CumColl 379 | Time    5.0s\n",
      "Episodes  101- 200 | Epsilon 0.37 | TotalReward   -19979 | TotColl  175 | TotRT  100 | AvgLoss 373.4657 | Steps 8,235 | CumColl 554 | Time    8.8s\n",
      "Episodes  201- 300 | Epsilon 0.22 | TotalReward    -6611 | TotColl  118 | TotRT  100 | AvgLoss 349.6415 | Steps 11,906 | CumColl 672 | Time   13.2s\n",
      "Episodes  301- 400 | Epsilon 0.14 | TotalReward     4305 | TotColl   68 | TotRT  100 | AvgLoss 310.4964 | Steps 14,991 | CumColl 740 | Time   16.9s\n",
      "Episodes  401- 500 | Epsilon 0.08 | TotalReward    -2929 | TotColl  105 | TotRT  100 | AvgLoss 312.9796 | Steps 17,820 | CumColl 845 | Time   20.3s\n",
      "Episodes  501- 600 | Epsilon 0.05 | TotalReward    -2421 | TotColl  109 | TotRT  100 | AvgLoss 325.8468 | Steps 20,371 | CumColl 954 | Time   23.3s\n",
      "Episodes  601- 700 | Epsilon 0.05 | TotalReward    10926 | TotColl   43 | TotRT  100 | AvgLoss 313.5191 | Steps 22,855 | CumColl 997 | Time   26.2s\n",
      "Episodes  701- 800 | Epsilon 0.05 | TotalReward    -1709 | TotColl  109 | TotRT  100 | AvgLoss 310.3779 | Steps 25,164 | CumColl 1,106 | Time   29.1s\n",
      "Episodes  801- 900 | Epsilon 0.05 | TotalReward    12151 | TotColl   39 | TotRT  100 | AvgLoss 300.2681 | Steps 27,973 | CumColl 1,145 | Time   32.2s\n",
      "Episodes  901-1000 | Epsilon 0.05 | TotalReward    10570 | TotColl   51 | TotRT   99 | AvgLoss 283.0046 | Steps 30,953 | CumColl 1,196 | Time   35.5s\n",
      "Episodes 1001-1100 | Epsilon 0.05 | TotalReward     7065 | TotColl   69 | TotRT  100 | AvgLoss 279.1538 | Steps 33,318 | CumColl 1,265 | Time   38.2s\n",
      "Episodes 1101-1200 | Epsilon 0.05 | TotalReward     9819 | TotColl   56 | TotRT  100 | AvgLoss 274.8311 | Steps 35,559 | CumColl 1,321 | Time   40.7s\n",
      "Episodes 1201-1300 | Epsilon 0.05 | TotalReward    12905 | TotColl   39 | TotRT  100 | AvgLoss 272.1413 | Steps 37,704 | CumColl 1,360 | Time   43.0s\n",
      "Episodes 1301-1400 | Epsilon 0.05 | TotalReward    13249 | TotColl   42 | TotRT  100 | AvgLoss 274.0798 | Steps 40,345 | CumColl 1,402 | Time   46.0s\n",
      "Episodes 1401-1500 | Epsilon 0.05 | TotalReward    12683 | TotColl   39 | TotRT  100 | AvgLoss 260.8183 | Steps 42,562 | CumColl 1,441 | Time   48.7s\n",
      "Episodes 1501-1600 | Epsilon 0.05 | TotalReward     6920 | TotColl   68 | TotRT  100 | AvgLoss 256.8790 | Steps 44,992 | CumColl 1,509 | Time   51.4s\n",
      "Episodes 1601-1700 | Epsilon 0.05 | TotalReward    13634 | TotColl   34 | TotRT  100 | AvgLoss 260.5474 | Steps 47,208 | CumColl 1,543 | Time   54.0s\n",
      "Episodes 1701-1800 | Epsilon 0.05 | TotalReward    12399 | TotColl   46 | TotRT  100 | AvgLoss 259.8454 | Steps 49,519 | CumColl 1,589 | Time   56.8s\n",
      "Episodes 1801-1900 | Epsilon 0.05 | TotalReward     8395 | TotColl   69 | TotRT  100 | AvgLoss 259.6269 | Steps 51,734 | CumColl 1,658 | Time   59.4s\n",
      "Episodes 1901-2000 | Epsilon 0.05 | TotalReward    15679 | TotColl   29 | TotRT  100 | AvgLoss 245.1906 | Steps 54,085 | CumColl 1,687 | Time   62.4s\n",
      "Episodes 2001-2100 | Epsilon 0.05 | TotalReward    12178 | TotColl   59 | TotRT  100 | AvgLoss 227.4627 | Steps 56,527 | CumColl 1,746 | Time   65.3s\n",
      "Episodes 2101-2200 | Epsilon 0.05 | TotalReward    11112 | TotColl   54 | TotRT  100 | AvgLoss 228.7831 | Steps 58,865 | CumColl 1,800 | Time   68.1s\n",
      "Episodes 2201-2300 | Epsilon 0.05 | TotalReward    18283 | TotColl   29 | TotRT  100 | AvgLoss 223.9490 | Steps 61,412 | CumColl 1,829 | Time   71.0s\n",
      "Episodes 2301-2400 | Epsilon 0.05 | TotalReward    17831 | TotColl   30 | TotRT  100 | AvgLoss 215.0366 | Steps 63,981 | CumColl 1,859 | Time   74.2s\n",
      "Episodes 2401-2500 | Epsilon 0.05 | TotalReward    13315 | TotColl   54 | TotRT  100 | AvgLoss 206.4839 | Steps 66,316 | CumColl 1,913 | Time   76.9s\n",
      "Episodes 2501-2600 | Epsilon 0.05 | TotalReward    12986 | TotColl   42 | TotRT  100 | AvgLoss 203.5049 | Steps 68,490 | CumColl 1,955 | Time   79.4s\n",
      "Episodes 2601-2700 | Epsilon 0.05 | TotalReward    11634 | TotColl   45 | TotRT  100 | AvgLoss 192.9822 | Steps 70,756 | CumColl 2,000 | Time   82.0s\n",
      "Episodes 2701-2800 | Epsilon 0.05 | TotalReward    15438 | TotColl   34 | TotRT  100 | AvgLoss 197.1982 | Steps 73,128 | CumColl 2,034 | Time   84.6s\n",
      "Episodes 2801-2900 | Epsilon 0.05 | TotalReward    17775 | TotColl   30 | TotRT  100 | AvgLoss 183.3971 | Steps 75,533 | CumColl 2,064 | Time   87.3s\n",
      "Episodes 2901-3000 | Epsilon 0.05 | TotalReward    16203 | TotColl   28 | TotRT  100 | AvgLoss 176.7335 | Steps 77,830 | CumColl 2,092 | Time   89.9s\n",
      "Episodes 3001-3100 | Epsilon 0.05 | TotalReward    11280 | TotColl   54 | TotRT  100 | AvgLoss 180.6732 | Steps 80,020 | CumColl 2,146 | Time   92.5s\n",
      "Episodes 3101-3200 | Epsilon 0.05 | TotalReward    11940 | TotColl   47 | TotRT  100 | AvgLoss 182.8183 | Steps 82,190 | CumColl 2,193 | Time   95.2s\n",
      "Episodes 3201-3300 | Epsilon 0.05 | TotalReward    20205 | TotColl   24 | TotRT  100 | AvgLoss 171.4772 | Steps 84,555 | CumColl 2,217 | Time   97.9s\n",
      "Episodes 3301-3400 | Epsilon 0.05 | TotalReward    15351 | TotColl   42 | TotRT  100 | AvgLoss 170.7586 | Steps 86,854 | CumColl 2,259 | Time  100.7s\n",
      "Episodes 3401-3500 | Epsilon 0.05 | TotalReward    15664 | TotColl   31 | TotRT  100 | AvgLoss 170.4403 | Steps 89,120 | CumColl 2,290 | Time  103.4s\n",
      "Episodes 3501-3600 | Epsilon 0.05 | TotalReward    13386 | TotColl   56 | TotRT  100 | AvgLoss 167.5600 | Steps 91,624 | CumColl 2,346 | Time  106.5s\n",
      "Episodes 3601-3700 | Epsilon 0.05 | TotalReward    16832 | TotColl   22 | TotRT  100 | AvgLoss 165.6570 | Steps 93,832 | CumColl 2,368 | Time  109.1s\n",
      "Episodes 3701-3800 | Epsilon 0.05 | TotalReward    16528 | TotColl   34 | TotRT  100 | AvgLoss 159.0542 | Steps 96,194 | CumColl 2,402 | Time  111.8s\n",
      "Episodes 3801-3900 | Epsilon 0.05 | TotalReward    18260 | TotColl   32 | TotRT  100 | AvgLoss 159.0802 | Steps 98,454 | CumColl 2,434 | Time  114.4s\n",
      "Episodes 3901-4000 | Epsilon 0.05 | TotalReward    20112 | TotColl   27 | TotRT  100 | AvgLoss 158.6177 | Steps 101,022 | CumColl 2,461 | Time  117.3s\n",
      "Episodes 4001-4100 | Epsilon 0.05 | TotalReward    17590 | TotColl   26 | TotRT  100 | AvgLoss 152.2814 | Steps 103,402 | CumColl 2,487 | Time  120.0s\n",
      "Episodes 4101-4200 | Epsilon 0.05 | TotalReward    16743 | TotColl   36 | TotRT  100 | AvgLoss 148.0059 | Steps 105,699 | CumColl 2,523 | Time  122.8s\n",
      "Episodes 4201-4300 | Epsilon 0.05 | TotalReward    19089 | TotColl   25 | TotRT  100 | AvgLoss 140.8911 | Steps 108,100 | CumColl 2,548 | Time  125.8s\n",
      "Episodes 4301-4400 | Epsilon 0.05 | TotalReward    19420 | TotColl   23 | TotRT  100 | AvgLoss 137.5395 | Steps 110,530 | CumColl 2,571 | Time  128.7s\n",
      "Episodes 4401-4500 | Epsilon 0.05 | TotalReward    15309 | TotColl   41 | TotRT  100 | AvgLoss 140.7914 | Steps 113,091 | CumColl 2,612 | Time  131.8s\n",
      "Episodes 4501-4600 | Epsilon 0.05 | TotalReward    17023 | TotColl   34 | TotRT  100 | AvgLoss 138.5789 | Steps 115,548 | CumColl 2,646 | Time  134.7s\n",
      "Episodes 4601-4700 | Epsilon 0.05 | TotalReward    15290 | TotColl   41 | TotRT  100 | AvgLoss 141.8283 | Steps 117,828 | CumColl 2,687 | Time  137.3s\n",
      "Episodes 4701-4800 | Epsilon 0.05 | TotalReward    19075 | TotColl   21 | TotRT  100 | AvgLoss 137.9508 | Steps 120,093 | CumColl 2,708 | Time  139.9s\n",
      "Episodes 4801-4900 | Epsilon 0.05 | TotalReward    18151 | TotColl   32 | TotRT  100 | AvgLoss 136.0016 | Steps 122,582 | CumColl 2,740 | Time  142.9s\n",
      "Episodes 4901-5000 | Epsilon 0.05 | TotalReward    16049 | TotColl   40 | TotRT  100 | AvgLoss 137.2250 | Steps 124,903 | CumColl 2,780 | Time  145.7s\n",
      "Episodes 5001-5100 | Epsilon 0.05 | TotalReward    17406 | TotColl   40 | TotRT  100 | AvgLoss 138.9192 | Steps 127,237 | CumColl 2,820 | Time  148.5s\n",
      "Episodes 5101-5200 | Epsilon 0.05 | TotalReward    15401 | TotColl   43 | TotRT  100 | AvgLoss 137.1755 | Steps 129,426 | CumColl 2,863 | Time  151.1s\n",
      "Episodes 5201-5300 | Epsilon 0.05 | TotalReward    19214 | TotColl   28 | TotRT  100 | AvgLoss 135.2343 | Steps 131,972 | CumColl 2,891 | Time  154.2s\n",
      "Episodes 5301-5400 | Epsilon 0.05 | TotalReward    19958 | TotColl   25 | TotRT  100 | AvgLoss 137.0293 | Steps 134,474 | CumColl 2,916 | Time  157.3s\n",
      "Episodes 5401-5500 | Epsilon 0.05 | TotalReward    15159 | TotColl   45 | TotRT  100 | AvgLoss 136.3331 | Steps 136,945 | CumColl 2,961 | Time  160.4s\n",
      "Episodes 5501-5600 | Epsilon 0.05 | TotalReward    17740 | TotColl   27 | TotRT  100 | AvgLoss 131.9642 | Steps 139,275 | CumColl 2,988 | Time  163.1s\n",
      "Episodes 5601-5700 | Epsilon 0.05 | TotalReward    14731 | TotColl   48 | TotRT  100 | AvgLoss 132.8158 | Steps 141,674 | CumColl 3,036 | Time  165.9s\n",
      "Episodes 5701-5800 | Epsilon 0.05 | TotalReward    15477 | TotColl   39 | TotRT  100 | AvgLoss 138.2558 | Steps 143,997 | CumColl 3,075 | Time  168.7s\n",
      "Episodes 5801-5900 | Epsilon 0.05 | TotalReward    16045 | TotColl   37 | TotRT  100 | AvgLoss 137.1287 | Steps 146,352 | CumColl 3,112 | Time  171.5s\n",
      "Episodes 5901-6000 | Epsilon 0.05 | TotalReward    19296 | TotColl   23 | TotRT  100 | AvgLoss 135.9667 | Steps 148,626 | CumColl 3,135 | Time  174.3s\n",
      "Episodes 6001-6100 | Epsilon 0.05 | TotalReward    13881 | TotColl   44 | TotRT  100 | AvgLoss 130.0675 | Steps 150,795 | CumColl 3,179 | Time  176.7s\n",
      "Episodes 6101-6200 | Epsilon 0.05 | TotalReward    20735 | TotColl   30 | TotRT  100 | AvgLoss 129.8130 | Steps 153,640 | CumColl 3,209 | Time  180.0s\n",
      "Episodes 6201-6300 | Epsilon 0.05 | TotalReward    13901 | TotColl   45 | TotRT  100 | AvgLoss 132.3122 | Steps 156,039 | CumColl 3,254 | Time  182.7s\n",
      "Episodes 6301-6400 | Epsilon 0.05 | TotalReward    10980 | TotColl   61 | TotRT  100 | AvgLoss 133.6990 | Steps 158,379 | CumColl 3,315 | Time  185.4s\n",
      "Episodes 6401-6500 | Epsilon 0.05 | TotalReward    15168 | TotColl   34 | TotRT  100 | AvgLoss 139.3121 | Steps 160,941 | CumColl 3,349 | Time  188.3s\n",
      "Episodes 6501-6600 | Epsilon 0.05 | TotalReward    14528 | TotColl   44 | TotRT  100 | AvgLoss 134.0916 | Steps 163,373 | CumColl 3,393 | Time  191.1s\n",
      "Episodes 6601-6700 | Epsilon 0.05 | TotalReward    16204 | TotColl   26 | TotRT  100 | AvgLoss 138.7535 | Steps 165,709 | CumColl 3,419 | Time  193.9s\n",
      "Episodes 6701-6800 | Epsilon 0.05 | TotalReward    16557 | TotColl   27 | TotRT  100 | AvgLoss 138.4647 | Steps 168,052 | CumColl 3,446 | Time  196.8s\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Logging utility to record training metrics\n",
    "class MetricLogger:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.collisions = []\n",
    "        self.round_trips = []\n",
    "        self.lengths = []\n",
    "    def log_episode(self, total_reward, collisions, round_trips, length):\n",
    "        self.rewards.append(total_reward)\n",
    "        self.collisions.append(collisions)\n",
    "        self.round_trips.append(round_trips)\n",
    "        self.lengths.append(length)\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"mean_reward\": np.mean(self.rewards) if self.rewards else 0.0,\n",
    "            \"mean_collisions\": np.mean(self.collisions) if self.collisions else 0.0,\n",
    "            \"mean_round_trips\": np.mean(self.round_trips) if self.round_trips else 0.0,\n",
    "            \"mean_length\": np.mean(self.lengths) if self.lengths else 0.0\n",
    "        }\n",
    "\n",
    "# Training parameters and budgets\n",
    "num_episodes = 8000\n",
    "max_steps_per_episode = 120   # safety cap on steps per episode (will often terminate earlier)\n",
    "buffer_capacity = 50000\n",
    "step_budget = 1_500_000       # max training steps (across all agents)\n",
    "collision_budget = 4000       # max total collisions during training\n",
    "walltime_budget = 600         # max wall-clock time (seconds) for training\n",
    "\n",
    "# Initialize environment, replay buffer, epsilon scheduler, and metric logger\n",
    "env = GridWorldEnvironment()\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "eps_scheduler = EpsilonScheduler(start=1.0, end=0.05, decay=0.995)\n",
    "logger = MetricLogger()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "# Training loop\n",
    "import time\n",
    "start_time = time.time()\n",
    "total_steps = 0\n",
    "total_collisions = 0\n",
    "\n",
    "# ------------ 100‑episode window accumulators ------------\n",
    "window_reward_sum     = 0.0\n",
    "window_coll_sum       = 0\n",
    "window_roundtrip_sum  = 0\n",
    "window_loss_sum       = 0.0\n",
    "window_loss_count     = 0\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    while True:\n",
    "        a_row = np.random.randint(env.n)\n",
    "        a_col = np.random.randint(env.m)\n",
    "        b_row = np.random.randint(env.n)\n",
    "        b_col = np.random.randint(env.m)\n",
    "        if (a_row, a_col) != (b_row, b_col):\n",
    "            break\n",
    "    env.food_source_location = (a_row, a_col)\n",
    "    env.nest_location = (b_row, b_col)\n",
    "    env.reset_environment_counters()\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_collisions = 0\n",
    "    episode_round_trips = 0\n",
    "    eps = eps_scheduler.get_epsilon()\n",
    "    done_episode = False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        for _ in range(env.num_agents):\n",
    "            agent_id = env.get_next_agent()\n",
    "            state = env.get_state(agent_id)\n",
    "\n",
    "            if random.random() < eps:\n",
    "                action = random.randrange(action_dim)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = int(torch.argmax(q_values).item())\n",
    "\n",
    "            reward = env.take_action(agent_id, action)\n",
    "            env.check_collisions()\n",
    "\n",
    "            if env.collision_count > episode_collisions:\n",
    "                new_collisions = env.collision_count - episode_collisions\n",
    "                total_collisions += new_collisions\n",
    "                episode_collisions = env.collision_count\n",
    "                for ag in env.agents:\n",
    "                    if ag.collision_penalty != 0:\n",
    "                        if ag.agent_id != agent_id:\n",
    "                            reward += ag.collision_penalty\n",
    "                        episode_reward += ag.collision_penalty\n",
    "                        ag.collision_penalty = 0\n",
    "\n",
    "            next_state = env.get_state(agent_id)\n",
    "            done = False\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                state_batch = torch.tensor(states, dtype=torch.float32)\n",
    "                next_state_batch = torch.tensor(next_states, dtype=torch.float32)\n",
    "                action_batch = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "                reward_batch = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "                done_batch = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                q_current = policy_net(state_batch).gather(1, action_batch)\n",
    "                q_next = target_net(next_state_batch).max(dim=1, keepdim=True)[0]\n",
    "                q_target = reward_batch + gamma * q_next * (1 - done_batch)\n",
    "\n",
    "                loss = loss_fn(q_current, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track loss in 100-episode window\n",
    "                window_loss_sum += loss.item()\n",
    "                window_loss_count += 1\n",
    "\n",
    "            total_steps += 1\n",
    "            if env.round_trip_count > 0:\n",
    "                done_episode = True\n",
    "            if total_steps >= step_budget or total_collisions >= collision_budget or (time.time() - start_time) >= walltime_budget:\n",
    "                done_episode = True\n",
    "            if done_episode:\n",
    "                break\n",
    "\n",
    "        if total_steps % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done_episode or step == max_steps_per_episode - 1:\n",
    "            episode_round_trips = env.round_trip_count\n",
    "            break\n",
    "\n",
    "    logger.log_episode(episode_reward, episode_collisions, episode_round_trips, step+1)\n",
    "\n",
    "    # --- Accumulate episode stats for current 100-episode window ---\n",
    "    window_reward_sum    += episode_reward\n",
    "    window_coll_sum      += episode_collisions\n",
    "    window_roundtrip_sum += episode_round_trips\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    # --- Print block every 100 episodes ---\n",
    "    if episode % 100 == 0:\n",
    "        avg_loss = (window_loss_sum / window_loss_count) if window_loss_count else float('nan')\n",
    "        print(f\"Ep {episode-99:4d}-{episode:4d} | \"\n",
    "              f\"Epsilon {eps:.2f} | \"\n",
    "              f\"Reward {window_reward_sum:8.0f} | \"\n",
    "              f\"Coll {window_coll_sum:4d} | \"\n",
    "              f\"RT {window_roundtrip_sum:4d} | \"\n",
    "              f\"AvgLoss {avg_loss:7.4f} | \"\n",
    "              f\"Steps {total_steps:,} | CumColl {total_collisions:,} | \"\n",
    "              f\"Time {time.time()-start_time:6.1f}s\") \n",
    "        # Reset accumulators\n",
    "        window_reward_sum     = 0.0 #\n",
    "        window_coll_sum       = 0\n",
    "        window_roundtrip_sum  = 0\n",
    "        window_loss_sum       = 0.0\n",
    "        window_loss_count     = 0\n",
    "\n",
    "    # Check stop conditions\n",
    "    if total_steps >= step_budget:\n",
    "        print(\"Stopping training early due to step budget limit reached.\")\n",
    "        break\n",
    "    elif total_collisions >= collision_budget:\n",
    "        print (\"Stopping training early due to collision budget limit reached.\")\n",
    "        break\n",
    "    elif (time.time() - start_time) >= walltime_budget:\n",
    "        print (\"Stopping training early due to time budget limit reached.\")\n",
    "        break\n",
    "\n",
    "# Save trained model\n",
    "torch.save(policy_net.state_dict(), \"trained_model.pth\")\n",
    "print(\"Training completed.\")\n",
    "stats = logger.get_stats()\n",
    "print(f\"Average Reward: {stats['mean_reward']:.2f}, \"\n",
    "      f\"Average Collisions: {stats['mean_collisions']:.2f}, \"\n",
    "      f\"Average RoundTrips: {stats['mean_round_trips']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15c5b962-72a4-458c-8bee-b383e0278f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 96.08%\n",
      "Success rate per distribution: {'1,3': 575, '2,2': 575, '3,1': 575, '4,0': 581}\n",
      "Failures breakdown: {'collisions': 21, 'timeout': 73, 'incomplete': 0}\n",
      "Average steps for successful episodes: 23.22\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of Trained Policy\n",
    "\n",
    "# Load the trained model for evaluation\n",
    "trained_model = nn.Sequential(\n",
    "    nn.Linear(state_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "trained_model.load_state_dict(torch.load(\"trained_model.pth\", map_location=\"cpu\"))\n",
    "trained_model.eval()\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"Evaluate the trained model on all possible A-B configurations and specified agent distributions.\"\"\"\n",
    "    GRID_SIZE = 5\n",
    "    distributions = [(1,3), (2,2), (3,1), (4,0)]  # (agents at B, agents at A)\n",
    "    success_count = 0\n",
    "    total_tests = 0\n",
    "    stats = {\n",
    "        'total_tests': 0,\n",
    "        'successful_tests': 0,\n",
    "        'failures': {'collisions': 0, 'timeout': 0, 'incomplete': 0},\n",
    "        'distribution_success': {'1,3': 0, '2,2': 0, '3,1': 0, '4,0': 0},\n",
    "        'avg_steps_successful': 0\n",
    "    }\n",
    "    # Helper: choose greedy action using the trained model\n",
    "    def greedy_action(state):\n",
    "        state_t = torch.tensor(state, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            q_vals = model(state_t)\n",
    "        return int(torch.argmax(q_vals).item())\n",
    "\n",
    "    # Iterate over all possible A, B placements\n",
    "    for ax in range(GRID_SIZE):\n",
    "        for ay in range(GRID_SIZE):\n",
    "            for bx in range(GRID_SIZE):\n",
    "                for by in range(GRID_SIZE):\n",
    "                    if (ax, ay) == (bx, by):\n",
    "                        continue  # skip invalid configs where A == B\n",
    "                    for (b_agents, a_agents) in distributions:\n",
    "                        total_tests += 1\n",
    "                        stats['total_tests'] = total_tests\n",
    "                        # Initialize env with given A and B\n",
    "                        test_env = GridWorldEnvironment(n=GRID_SIZE, m=GRID_SIZE, num_agents=4,\n",
    "                                                        food_source_location=(ax, ay),\n",
    "                                                        nest_location=(bx, by))\n",
    "                        # Manually set agents at B or A according to the distribution\n",
    "                        # First agent (index 0) at B, since we test that agent's ability to do B->A->B\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            if i < b_agents:\n",
    "                                # place this agent at B (nest), without item\n",
    "                                test_env.agents[i].position = test_env.nest_location\n",
    "                                test_env.agents[i].has_item = 0\n",
    "                                test_env.agents[i].direction = False\n",
    "                            else:\n",
    "                                # place this agent at A (food source), with item\n",
    "                                test_env.agents[i].position = test_env.food_source_location\n",
    "                                test_env.agents[i].has_item = 1\n",
    "                                test_env.agents[i].direction = True\n",
    "                        # Reset central clock and counters for safety\n",
    "                        test_env.clock = 0\n",
    "                        test_env.collision_count = 0\n",
    "                        test_env.round_trip_count = 0\n",
    "                        for i in range(test_env.num_agents):\n",
    "                            test_env.agent_states[i] = 0\n",
    "                            test_env.agent_round_trips[i] = False\n",
    "\n",
    "                        # Simulate until first agent (index 0) completes B->A->B or until 25 steps per agent\n",
    "                        agent0_success = False\n",
    "                        collided = False\n",
    "                        max_steps = 25  # max moves per agent\n",
    "                        steps_taken = 0\n",
    "                        for t in range(max_steps * test_env.num_agents):\n",
    "                            aid = test_env.get_next_agent()\n",
    "                            state = test_env.get_state(aid)\n",
    "                            action = greedy_action(state)\n",
    "                            test_env.take_action(aid, action)\n",
    "                            test_env.check_collisions()\n",
    "                            # If any collision occurs, note if agent0 was involved\n",
    "                            if test_env.collision_count > 0:\n",
    "                                for ag in test_env.agents:\n",
    "                                    if ag.collision_penalty != 0:\n",
    "                                        if ag.agent_id == 0:\n",
    "                                            collided = True\n",
    "                                        ag.collision_penalty = 0\n",
    "                            # Check if agent 0 completed B->A->B (round trip)\n",
    "                            # B->A->B is completed when agent0 goes from B (start) -> A (pickup) -> B (drop-off).\n",
    "                            # In our env logic, that corresponds to agent_states[0] reaching 3 (completed) or round_trip_count increment.\n",
    "                            if test_env.agent_round_trips[0]:\n",
    "                                agent0_success = True\n",
    "                                break\n",
    "                            steps_taken += 1\n",
    "                        # Determine success/failure for this test\n",
    "                        if agent0_success and not collided:\n",
    "                            success_count += 1\n",
    "                            stats['successful_tests'] += 1\n",
    "                            stats['distribution_success'][f'{b_agents},{a_agents}'] += 1\n",
    "                            stats['avg_steps_successful'] += steps_taken\n",
    "                        else:\n",
    "                            # Determine failure reason\n",
    "                            if not agent0_success and steps_taken >= max_steps * test_env.num_agents:\n",
    "                                stats['failures']['timeout'] += 1\n",
    "                            elif collided:\n",
    "                                stats['failures']['collisions'] += 1\n",
    "                            else:\n",
    "                                stats['failures']['incomplete'] += 1\n",
    "\n",
    "    # Calculate success rate and average steps\n",
    "    success_rate = success_count / total_tests\n",
    "    if stats['successful_tests'] > 0:\n",
    "        stats['avg_steps_successful'] /= stats['successful_tests']\n",
    "    return success_rate, stats\n",
    "\n",
    "# Run evaluation\n",
    "success_rate, eval_stats = evaluate_model(trained_model)\n",
    "print(f\"Success Rate: {success_rate*100:.2f}%\")\n",
    "print(\"Success rate per distribution:\", {dist: eval_stats['distribution_success'][dist] for dist in eval_stats['distribution_success']})\n",
    "print(\"Failures breakdown:\", eval_stats['failures'])\n",
    "print(f\"Average steps for successful episodes: {eval_stats['avg_steps_successful']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3af05-83d2-40c6-a8ed-5fa605c70d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
