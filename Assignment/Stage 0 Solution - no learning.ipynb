{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c6433ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define the Q-table agent - currently just a hook without learning\n",
    "class QTableAgent:\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # hook for the policy\n",
    "        return np.random.randint(num_actions)\n",
    "\n",
    "\n",
    "# Define the grid world environment\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, n, m):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.agent_position = (0, 0)  # Start at the top-left corner of the grid world\n",
    "        self.reached_A = False\n",
    "        # fixed food source location\n",
    "        # self.food_source_location = (4, 4)\n",
    "        # dynamic changing food source location\n",
    "        self.food_source_location = (np.random.randint(n), np.random.randint(m))\n",
    "        self.nest_location = (n-1,m-1)\n",
    "        self.rewards = np.zeros((grid_rows, grid_cols))\n",
    "        self.rewards[self.food_source_location[0],self.food_source_location[1]] = 10\n",
    "        self.rewards[self.nest_location[0],self.nest_location[1]] = 50\n",
    "     \n",
    "    \n",
    "    def _reset(self):\n",
    "        self.agent_position = (0, 0)  # Start at the top-left corner of the grid world\n",
    "        self.reached_A = False\n",
    "        #random the location of food source per episode; commands off this line for the fix food source\n",
    "        self.food_source_location = (np.random.randint(self.n), np.random.randint(self.m))\n",
    "        self.rewards = np.zeros((grid_rows, grid_cols))\n",
    "        self.rewards[self.food_source_location[0],self.food_source_location[1]] = 10\n",
    "        self.rewards[self.nest_location[0],self.nest_location[1]] = 50\n",
    "      \n",
    " \n",
    "    def get_state(self):\n",
    "\n",
    "        reached_A_state = 1 if self.reached_A else 0\n",
    "        return self.agent_position[0], self.agent_position[1], self.food_source_location[0], self.food_source_location[1], reached_A_state\n",
    "\n",
    "    def check_done(self):\n",
    "        if self.agent_position == self.nest_location and self.reached_A:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        row, col = self.agent_position\n",
    "\n",
    "        # Perform the chosen action and observe the next state and reward\n",
    "        if action == 0:  # Up\n",
    "            next_position = (max(row - 1, 0), col)\n",
    "        elif action == 1:  # Down\n",
    "            next_position = (min(row + 1, grid_rows - 1), col)\n",
    "        elif action == 2:  # Left\n",
    "            next_position = (row, max(col - 1, 0))\n",
    "        elif action == 3:  # Right\n",
    "            next_position = (row, min(col + 1, grid_cols - 1))\n",
    "       \n",
    "        if self.reached_A == False: \n",
    "            if next_position == self.food_source_location:\n",
    "                self.reached_A = True\n",
    "                reward = self.rewards[next_position]\n",
    "            else:\n",
    "                 reward = -1\n",
    "        elif self.reached_A and next_position == self.nest_location:\n",
    "            reward = self.rewards[next_position]\n",
    "        \n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        self.agent_position = next_position\n",
    "        return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ddb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episodes\n",
      "5000 episodes\n",
      "Finished in  1.772093  seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import process_time\n",
    "\n",
    "t = process_time()\n",
    "\n",
    "num_episodes = 5000\n",
    "max_steps = 100 \n",
    "\n",
    "# Define the grid world dimensions\n",
    "grid_rows = 5\n",
    "grid_cols = 5\n",
    "\n",
    "# Define the number of actions (up, down, left, right)\n",
    "num_actions = 4\n",
    "\n",
    "# Create the Q-table agent and grid world environment\n",
    "agent = QTableAgent()\n",
    "environment = GridWorldEnvironment(grid_rows,grid_cols)\n",
    "\n",
    "reward_total = []\n",
    "\n",
    "for episode in range(num_episodes+1):   \n",
    "    environment._reset()\n",
    "    number_of_steps = 0\n",
    "    reward_per_episode =  0\n",
    "    while number_of_steps<= max_steps and (environment.agent_position != environment.nest_location or environment.reached_A != True):  \n",
    "        # Continue until reaching location B or too many steps   \n",
    "        state = environment.get_state()\n",
    "        action = agent.choose_action(state)\n",
    "        reward = environment.take_action(action)\n",
    "        reward_per_episode += reward\n",
    "        done = environment.check_done()\n",
    "        number_of_steps += 1\n",
    "    reward_total.append(reward_per_episode)\n",
    "    if (episode%5000==0):\n",
    "        print(episode, \"episodes\")\n",
    "\n",
    "elapsed_time = process_time() - t\n",
    "\n",
    "\n",
    "print(\"Finished in \", elapsed_time, \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c977e18-00a5-479c-a37d-7a6eccd16dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
